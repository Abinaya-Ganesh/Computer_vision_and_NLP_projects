{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCr0VcdLfXFw"
      },
      "source": [
        "LeNet5, AlexNet, VGG, ResNet18, SENet18 and GoogleNet Architrctures are defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNY9GvjJDko5"
      },
      "source": [
        "**LeNet5 ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aLX1kiFU13FO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwEAdnHDrjw"
      },
      "source": [
        "**AlexNet ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "87DGAxLTCSnh"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8M638EgDvc1"
      },
      "source": [
        "**VGG ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MuJbk66DguaO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogTm6rQ_EQBF"
      },
      "source": [
        "**ResNet18 ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zQbwBqMF2Xqt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride):\n",
        "        strides = [stride] + [1]*(blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, (1,1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-uTN-zkLt85"
      },
      "source": [
        "**SENet18 ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ku79cSXsCvR3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
        "        y = F.relu(self.fc1(y))\n",
        "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.se = SEBlock(planes, reduction)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class SENet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(SENet18, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(SEBasicBlock, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(SEBasicBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(SEBasicBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(SEBasicBlock, 512, 2, stride=2)\n",
        "        self.linear = nn.Linear(512 * SEBasicBlock.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIZoq3o0ECfL"
      },
      "source": [
        "**GoogleNet ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "es5cLvv1CwYK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
        "        super(Inception, self).__init__()\n",
        "        self.branch1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
        "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
        "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        outputs = branch1, branch2, branch3, branch4\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "class GoogleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3, aux_logits=True):\n",
        "        super(GoogleNet, self).__init__()\n",
        "        self.aux_logits = aux_logits\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        if aux_logits:\n",
        "            self.aux1 = InceptionAux(512, num_classes)\n",
        "            self.aux2 = InceptionAux(528, num_classes)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.inception4a(x)\n",
        "        if self.aux_logits and self.training:\n",
        "            aux1 = self.aux1(x)\n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        if self.aux_logits and self.training:\n",
        "            aux2 = self.aux2(x)\n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            return torch.cat((aux1, aux2, x), dim=1)\n",
        "        return x\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(InceptionAux, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(128*4*4, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x), inplace=True)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds_xKhxKB3O0"
      },
      "source": [
        "**DOWNLOADING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JoHtzqH2nZoz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_datasets():\n",
        "    transform_mnist = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    transform_cifar = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "\n",
        "    fmnist_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "    fmnist_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "\n",
        "    cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
        "    cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "\n",
        "    return {\n",
        "        'MNIST': (mnist_train, mnist_test),\n",
        "        'FMNIST': (fmnist_train, fmnist_test),\n",
        "        'CIFAR-10': (cifar10_train, cifar10_test)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXVB20qyyrdL",
        "outputId": "5962f787-ee26-421f-cb98-6b698a0c0740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 4970912.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 159303.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1512725.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4253299.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8651709.98it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 58704.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:09<00:00, 478358.32it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 25522786.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16246578.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Function call for getting the data\n",
        "data = get_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sxQeGkp1iDN",
        "outputId": "18d713b0-d5c2-4703-ed77-f1ef0c4c648b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tipMoHicB6_Z"
      },
      "source": [
        "**USER DEFINED FUNCTION FOR TRAINING AND EVALUATING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0BDMjN4-Arq6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def training_and_evaluating(model, train_loader, test_loader, num_epochs):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "  train_loss_list = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    #getting the loss curve\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    #calculating accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "  #other evaluation metrics\n",
        "  precision = precision_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "  recall = recall_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "  f1 = f1_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "\n",
        "  print(f'\\nAccuracy: {accuracy:.2f}%')\n",
        "  print(f'Precision: {precision:.2f}')\n",
        "  print(f'Recall: {recall:.2f}')\n",
        "  print(f'F1-score: {f1:.2f}')\n",
        "\n",
        "  metrics = {'Accuracy':accuracy, 'Precision':precision, 'Recall':recall, 'F1-score':f1 }\n",
        "\n",
        "  return train_loss_list, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DdbPa4tBceL"
      },
      "source": [
        "**MNIST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opuxRUbwBKTs"
      },
      "outputs": [],
      "source": [
        "#creating a list for evaluation metrics of all models for MNIST Dataset\n",
        "metrics_m = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRyttVYAuA1C"
      },
      "outputs": [],
      "source": [
        "#Data_loader for MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['MNIST'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['MNIST'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VdcZatPizlp"
      },
      "source": [
        "**Function call for each model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t89XxG7RBjn9",
        "outputId": "8e868e42-6928-42e1-f47b-6a9ee5c66ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.2037\n",
            "Epoch [2/10], Loss: 0.0610\n",
            "Epoch [3/10], Loss: 0.0446\n",
            "Epoch [4/10], Loss: 0.0362\n",
            "Epoch [5/10], Loss: 0.0295\n",
            "Epoch [6/10], Loss: 0.0239\n",
            "Epoch [7/10], Loss: 0.0220\n",
            "Epoch [8/10], Loss: 0.0187\n",
            "Epoch [9/10], Loss: 0.0156\n",
            "Epoch [10/10], Loss: 0.0144\n",
            "\n",
            "Accuracy: 98.94%\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "F1-score: 0.99\n"
          ]
        }
      ],
      "source": [
        "#parameters of training_and_evaluating fumction --> model, train_loader, test_loader, num_epochs\n",
        "\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "m_train_loss_list_1, m_metrics_1 = training_and_evaluating(model_1, train_loader, test_loader, 10)\n",
        "m_metrics_1['Dataset']='MNIST'\n",
        "m_metrics_1['Model name']='LeNet5'\n",
        "metrics_m.append(m_metrics_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXg5kOgbRwqO",
        "outputId": "2604924f-5b8d-4ae4-ed41-a0674366430a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.3693\n",
            "Epoch [2/10], Loss: 0.1263\n",
            "Epoch [3/10], Loss: 0.1050\n",
            "Epoch [4/10], Loss: 0.0920\n",
            "Epoch [5/10], Loss: 0.0753\n",
            "Epoch [6/10], Loss: 0.0585\n",
            "Epoch [7/10], Loss: 0.0726\n",
            "Epoch [8/10], Loss: 0.0682\n",
            "Epoch [9/10], Loss: 0.0575\n",
            "Epoch [10/10], Loss: 0.0459\n",
            "\n",
            "Accuracy: 98.22%\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "F1-score: 0.99\n"
          ]
        }
      ],
      "source": [
        "model_2 = AlexNet(num_classes=10).to(device)\n",
        "m_train_loss_list_2, m_metrics_2 = training_and_evaluating(model_2, train_loader, test_loader, 10)\n",
        "m_metrics_2['Dataset']='MNIST'\n",
        "m_metrics_2['Model name']='AlexNet'\n",
        "metrics_m.append(m_metrics_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7JnH2Y3hCsK",
        "outputId": "1b94d69d-774c-4dd0-8baf-7dc5e0453e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.8652\n",
            "Epoch [2/10], Loss: 0.1503\n",
            "Epoch [3/10], Loss: 0.1338\n",
            "Epoch [4/10], Loss: 0.0840\n",
            "Epoch [5/10], Loss: 0.0774\n",
            "Epoch [6/10], Loss: 0.0792\n",
            "Epoch [7/10], Loss: 0.0743\n",
            "Epoch [8/10], Loss: 0.0607\n",
            "Epoch [9/10], Loss: 0.0578\n",
            "Epoch [10/10], Loss: 0.0489\n",
            "\n",
            "Accuracy: 98.53%\n",
            "Precision: 0.99\n",
            "Recall: 0.98\n",
            "F1-score: 0.99\n"
          ]
        }
      ],
      "source": [
        "model_3 = VGG(num_classes=10).to(device)\n",
        "m_train_loss_list_3, m_metrics_3 = training_and_evaluating(model_3, train_loader, test_loader, 10)\n",
        "m_metrics_3['Dataset']='MNIST'\n",
        "m_metrics_3['Model name']='VGG'\n",
        "metrics_m.append(m_metrics_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-CwdNO0Al7v",
        "outputId": "29d325de-c890-4088-faca-2ded5f2c1048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.1330\n",
            "Epoch [2/10], Loss: 0.0605\n",
            "Epoch [3/10], Loss: 0.0464\n",
            "Epoch [4/10], Loss: 0.0393\n",
            "Epoch [5/10], Loss: 0.0356\n",
            "Epoch [6/10], Loss: 0.0301\n",
            "Epoch [7/10], Loss: 0.0285\n",
            "Epoch [8/10], Loss: 0.0230\n",
            "Epoch [9/10], Loss: 0.0243\n",
            "Epoch [10/10], Loss: 0.0206\n",
            "\n",
            "Accuracy: 99.43%\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "F1-score: 0.99\n"
          ]
        }
      ],
      "source": [
        "model_4 = ResNet18(num_classes=10).to(device)\n",
        "m_train_loss_list_4, m_metrics_4 = training_and_evaluating(model_4, train_loader, test_loader, 10)\n",
        "m_metrics_4['Dataset'] ='MNIST'\n",
        "m_metrics_4['Model name'] = 'ResNet18'\n",
        "metrics_m.append(m_metrics_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUenEqE7AmOq",
        "outputId": "0876874e-c316-45f8-e896-5e0d6f86076f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.1169\n",
            "Epoch [2/10], Loss: 0.0530\n",
            "Epoch [3/10], Loss: 0.0407\n",
            "Epoch [4/10], Loss: 0.0350\n",
            "Epoch [5/10], Loss: 0.0324\n",
            "Epoch [6/10], Loss: 0.0264\n",
            "Epoch [7/10], Loss: 0.0239\n",
            "Epoch [8/10], Loss: 0.0212\n",
            "Epoch [9/10], Loss: 0.0191\n",
            "Epoch [10/10], Loss: 0.0158\n",
            "\n",
            "Accuracy: 99.16%\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "F1-score: 0.99\n"
          ]
        }
      ],
      "source": [
        "model_5 = SENet18(num_classes=10).to(device)\n",
        "m_train_loss_list_5, m_metrics_5 = training_and_evaluating(model_5, train_loader, test_loader, 10)\n",
        "m_metrics_5['Dataset']='MNIST'\n",
        "m_metrics_5['Model name']='SENet18'\n",
        "metrics_m.append(m_metrics_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLp04b9rCOwx",
        "outputId": "7400a14d-2ba5-4870-ffef-a2a71d23208f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.3853\n",
            "Epoch [2/10], Loss: 0.0956\n",
            "Epoch [3/10], Loss: 13.7884\n",
            "Epoch [4/10], Loss: 2.3013\n",
            "Epoch [5/10], Loss: 2.3013\n",
            "Epoch [6/10], Loss: 2.3013\n",
            "Epoch [7/10], Loss: 2.3013\n",
            "Epoch [8/10], Loss: 2.3013\n",
            "Epoch [9/10], Loss: 2.3013\n",
            "Epoch [10/10], Loss: 2.3013\n",
            "\n",
            "Accuracy: 9.80%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_6 = GoogleNet(num_classes=10).to(device)\n",
        "m_train_loss_list_6, m_metrics_6 = training_and_evaluating(model_6, train_loader, test_loader, 10)\n",
        "m_metrics_6['Dataset']='MNIST'\n",
        "m_metrics_6['Model name']='GoogleNet'\n",
        "metrics_m.append(m_metrics_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcT_vbBkiHO1"
      },
      "source": [
        "**Creating CSV files**\n",
        "\n",
        "Creating a loss dictionary for every dataset and converting them to csv files, so that we can download the csv file for particular dataset before runtime is exhausted and use it for plotting.\n",
        "\n",
        "The evaluation metrics are also loaded in a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6bGDn_O_9ZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "m_loss = {}\n",
        "m_loss['LeNet5'] = m_train_loss_list_1\n",
        "m_loss['AlexNet'] = m_train_loss_list_2\n",
        "m_loss['VGG'] = m_train_loss_list_3\n",
        "m_loss['ResNet18'] = m_train_loss_list_4\n",
        "m_loss['SENet18'] = m_train_loss_list_5\n",
        "m_loss['GoogleNet'] = m_train_loss_list_6\n",
        "\n",
        "df_m_loss = pd.DataFrame(m_loss)\n",
        "df_m_loss.to_csv(\"mnist_loss.csv\",index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmBB2ba-A7jr"
      },
      "outputs": [],
      "source": [
        "df_m_metrics = pd.DataFrame(metrics_m)\n",
        "df_m_metrics.to_csv(\"mnist_metrics.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZbUT0nwG2aM"
      },
      "source": [
        "**FASHION MNIST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqBMxBEMPQbM"
      },
      "outputs": [],
      "source": [
        "#creating a list for evaluation metrics of all models for Fashion MNIST Dataset\n",
        "metrics_f = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifoCJ792F4Po"
      },
      "outputs": [],
      "source": [
        "#Data_loader for FASHION MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['FMNIST'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['FMNIST'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNUKo8jjMDJ"
      },
      "source": [
        "**Function call for each model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_t9RHQ7G0U3",
        "outputId": "50487cf0-ba06-46bb-efc7-27ee0d89d044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.5395\n",
            "Epoch [2/10], Loss: 0.3505\n",
            "Epoch [3/10], Loss: 0.3092\n",
            "Epoch [4/10], Loss: 0.2838\n",
            "Epoch [5/10], Loss: 0.2622\n",
            "Epoch [6/10], Loss: 0.2453\n",
            "Epoch [7/10], Loss: 0.2318\n",
            "Epoch [8/10], Loss: 0.2200\n",
            "Epoch [9/10], Loss: 0.2059\n",
            "Epoch [10/10], Loss: 0.1956\n",
            "\n",
            "Accuracy: 90.43%\n",
            "Precision: 0.91\n",
            "Recall: 0.91\n",
            "F1-score: 0.91\n"
          ]
        }
      ],
      "source": [
        "#parameters of training_and_evaluating fumction --> model, train_loader, test_loader, num_epochs\n",
        "\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "f_train_loss_list_1, f_metrics_1 = training_and_evaluating(model_1, train_loader, test_loader, 10)\n",
        "f_metrics_1['Dataset']='FASHION MNIST'\n",
        "f_metrics_1['Model name']='LeNet5'\n",
        "metrics_f.append(f_metrics_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To3Edb85G0U7",
        "outputId": "ef2746b7-32cd-4bc5-af56-180bded58f01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.6522\n",
            "Epoch [2/10], Loss: 0.4300\n",
            "Epoch [3/10], Loss: 0.3843\n",
            "Epoch [4/10], Loss: 0.3579\n",
            "Epoch [5/10], Loss: 0.3475\n",
            "Epoch [6/10], Loss: 0.3169\n",
            "Epoch [7/10], Loss: 0.3087\n",
            "Epoch [8/10], Loss: 0.2907\n",
            "Epoch [9/10], Loss: 0.2840\n",
            "Epoch [10/10], Loss: 0.2683\n",
            "\n",
            "Accuracy: 89.14%\n",
            "Precision: 0.90\n",
            "Recall: 0.91\n",
            "F1-score: 0.90\n"
          ]
        }
      ],
      "source": [
        "model_2 = AlexNet(num_classes=10).to(device)\n",
        "f_train_loss_list_2, f_metrics_2 = training_and_evaluating(model_2, train_loader, test_loader, 10)\n",
        "f_metrics_2['Dataset']='FASHION MNIST'\n",
        "f_metrics_2['Model name']='AlexNet'\n",
        "metrics_f.append(f_metrics_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaBNM6y7G0U7",
        "outputId": "6edbe3fc-334a-4ad1-b6c0-7a3f0ae5a565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.3334\n",
            "Epoch [2/10], Loss: 0.5549\n",
            "Epoch [3/10], Loss: 0.4125\n",
            "Epoch [4/10], Loss: 0.3572\n",
            "Epoch [5/10], Loss: 0.3219\n",
            "Epoch [6/10], Loss: 0.3245\n",
            "Epoch [7/10], Loss: 0.2730\n",
            "Epoch [8/10], Loss: 0.2784\n",
            "Epoch [9/10], Loss: 0.2627\n",
            "Epoch [10/10], Loss: 0.2568\n",
            "\n",
            "Accuracy: 90.74%\n",
            "Precision: 0.92\n",
            "Recall: 0.92\n",
            "F1-score: 0.92\n"
          ]
        }
      ],
      "source": [
        "model_3 = VGG(num_classes=10).to(device)\n",
        "f_train_loss_list_3, f_metrics_3 = training_and_evaluating(model_3, train_loader, test_loader, 10)\n",
        "f_metrics_3['Dataset']='FASHION MNIST'\n",
        "f_metrics_3['Model name']='VGG'\n",
        "metrics_f.append(f_metrics_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcVpm0kYG0U8",
        "outputId": "03cf9453-99f1-464e-9a7d-42e58903f24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.4311\n",
            "Epoch [2/10], Loss: 0.3106\n",
            "Epoch [3/10], Loss: 0.2703\n",
            "Epoch [4/10], Loss: 0.2453\n",
            "Epoch [5/10], Loss: 0.2299\n",
            "Epoch [6/10], Loss: 0.2118\n",
            "Epoch [7/10], Loss: 0.1986\n",
            "Epoch [8/10], Loss: 0.1833\n",
            "Epoch [9/10], Loss: 0.1693\n",
            "Epoch [10/10], Loss: 0.1533\n",
            "\n",
            "Accuracy: 92.05%\n",
            "Precision: 0.93\n",
            "Recall: 0.94\n",
            "F1-score: 0.93\n"
          ]
        }
      ],
      "source": [
        "model_4 = ResNet18(num_classes=10).to(device)\n",
        "f_train_loss_list_4, f_metrics_4 = training_and_evaluating(model_4, train_loader, test_loader, 10)\n",
        "f_metrics_4['Dataset']='FASHION MNIST'\n",
        "f_metrics_4['Model name']='ResNet18'\n",
        "metrics_f.append(f_metrics_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5woM-jBG0U9",
        "outputId": "b8312e9c-e2d9-4e38-e635-d9bb7e4c70f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.4080\n",
            "Epoch [2/10], Loss: 0.2967\n",
            "Epoch [3/10], Loss: 0.2585\n",
            "Epoch [4/10], Loss: 0.2357\n",
            "Epoch [5/10], Loss: 0.2142\n",
            "Epoch [6/10], Loss: 0.1934\n",
            "Epoch [7/10], Loss: 0.1781\n",
            "Epoch [8/10], Loss: 0.1646\n",
            "Epoch [9/10], Loss: 0.1491\n",
            "Epoch [10/10], Loss: 0.1328\n",
            "\n",
            "Accuracy: 91.71%\n",
            "Precision: 0.93\n",
            "Recall: 0.93\n",
            "F1-score: 0.93\n"
          ]
        }
      ],
      "source": [
        "model_5 = SENet18(num_classes=10).to(device)\n",
        "f_train_loss_list_5, f_metrics_5 = training_and_evaluating(model_5, train_loader, test_loader, 10)\n",
        "f_metrics_5['Dataset']='FASHION MNIST'\n",
        "f_metrics_5['Model name']='SENet18'\n",
        "metrics_f.append(f_metrics_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltYYbS3bG0U-",
        "outputId": "745d69d9-fcdd-4414-df95-2cc9e0a1a46d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.6461\n",
            "Epoch [2/10], Loss: 0.3755\n",
            "Epoch [3/10], Loss: 0.3464\n",
            "Epoch [4/10], Loss: 0.3411\n",
            "Epoch [5/10], Loss: 0.3385\n",
            "Epoch [6/10], Loss: 0.3358\n",
            "Epoch [7/10], Loss: 0.3597\n",
            "Epoch [8/10], Loss: 0.3280\n",
            "Epoch [9/10], Loss: 18169062271.1433\n",
            "Epoch [10/10], Loss: 2.3034\n",
            "\n",
            "Accuracy: 10.00%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_6 = GoogleNet(num_classes=10).to(device)\n",
        "f_train_loss_list_6, f_metrics_6 = training_and_evaluating(model_6, train_loader, test_loader, 10)\n",
        "f_metrics_6['Dataset']='FASHION MNIST'\n",
        "f_metrics_6['Model name']='GoogleNet'\n",
        "metrics_f.append(f_metrics_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKri3MTTjooN"
      },
      "source": [
        "**Creating CSV File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "771BPSrgLHnB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "f_loss = {}\n",
        "f_loss['LeNet5'] = f_train_loss_list_1\n",
        "f_loss['AlexNet'] = f_train_loss_list_2\n",
        "f_loss['VGG'] = f_train_loss_list_3\n",
        "f_loss['ResNet18'] = f_train_loss_list_4\n",
        "f_loss['SENet18'] = f_train_loss_list_5\n",
        "f_loss['GoogleNet'] = f_train_loss_list_6\n",
        "\n",
        "df_f_loss = pd.DataFrame(f_loss)\n",
        "df_f_loss.to_csv(\"fashion_mnist_loss.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVdDr0lS_B90"
      },
      "outputs": [],
      "source": [
        "df_f_metrics = pd.DataFrame(metrics_f)\n",
        "df_f_metrics.to_csv(\"fashion_mnist_metrics.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqMUBnkQHKXA"
      },
      "source": [
        "**CIFAR-10 DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gkBJecv-BqV_"
      },
      "outputs": [],
      "source": [
        "#creating a list for evaluation metrics of all models for CIFAR-10 Dataset\n",
        "metrics_c = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OgXTjyJaHWQK"
      },
      "outputs": [],
      "source": [
        "#Data_loader for CIFAR-10 DATA\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['CIFAR-10'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['CIFAR-10'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCds1xvqjOjG"
      },
      "source": [
        "**Function call for each model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_ULpVGhHWQL",
        "outputId": "12ba1d8a-e994-4f6a-aa8d-933dd4478ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.6633\n",
            "Epoch [2/20], Loss: 1.3550\n",
            "Epoch [3/20], Loss: 1.2386\n",
            "Epoch [4/20], Loss: 1.1604\n",
            "Epoch [5/20], Loss: 1.0993\n",
            "Epoch [6/20], Loss: 1.0486\n",
            "Epoch [7/20], Loss: 1.0060\n",
            "Epoch [8/20], Loss: 0.9651\n",
            "Epoch [9/20], Loss: 0.9309\n",
            "Epoch [10/20], Loss: 0.8931\n",
            "Epoch [11/20], Loss: 0.8649\n",
            "Epoch [12/20], Loss: 0.8423\n",
            "Epoch [13/20], Loss: 0.8143\n",
            "Epoch [14/20], Loss: 0.7915\n",
            "Epoch [15/20], Loss: 0.7656\n",
            "Epoch [16/20], Loss: 0.7477\n",
            "Epoch [17/20], Loss: 0.7283\n",
            "Epoch [18/20], Loss: 0.7052\n",
            "Epoch [19/20], Loss: 0.6906\n",
            "Epoch [20/20], Loss: 0.6733\n",
            "\n",
            "Accuracy: 63.98%\n",
            "Precision: 0.63\n",
            "Recall: 0.63\n",
            "F1-score: 0.63\n"
          ]
        }
      ],
      "source": [
        "#parameters of training_and_evaluating fumction --> model, train_loader, test_loader, num_epochs\n",
        "\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "c_train_loss_list_1, c_metrics_1 = training_and_evaluating(model_1, train_loader, test_loader, 20)\n",
        "c_metrics_1['Dataset']='CIFAR-10'\n",
        "c_metrics_1['Model name']='LeNet5'\n",
        "metrics_c.append(c_metrics_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dLBHYd0HWQN",
        "outputId": "92b9d920-5e5e-4456-b442-78d12fb0df14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.8664\n",
            "Epoch [2/20], Loss: 1.5385\n",
            "Epoch [3/20], Loss: 1.3848\n",
            "Epoch [4/20], Loss: 1.2809\n",
            "Epoch [5/20], Loss: 1.1963\n",
            "Epoch [6/20], Loss: 1.1298\n",
            "Epoch [7/20], Loss: 1.0704\n",
            "Epoch [8/20], Loss: 1.0084\n",
            "Epoch [9/20], Loss: 0.9589\n",
            "Epoch [10/20], Loss: 0.9138\n",
            "Epoch [11/20], Loss: 0.8679\n",
            "Epoch [12/20], Loss: 0.8332\n",
            "Epoch [13/20], Loss: 0.7835\n",
            "Epoch [14/20], Loss: 0.7430\n",
            "Epoch [15/20], Loss: 0.7023\n",
            "Epoch [16/20], Loss: 0.6748\n",
            "Epoch [17/20], Loss: 0.6274\n",
            "Epoch [18/20], Loss: 0.5994\n",
            "Epoch [19/20], Loss: 0.5791\n",
            "Epoch [20/20], Loss: 0.5342\n",
            "\n",
            "Accuracy: 61.75%\n",
            "Precision: 0.60\n",
            "Recall: 0.60\n",
            "F1-score: 0.59\n"
          ]
        }
      ],
      "source": [
        "model_2 = AlexNet(num_classes=10).to(device)\n",
        "c_train_loss_list_2, c_metrics_2= training_and_evaluating(model_2, train_loader, test_loader, 20)\n",
        "c_metrics_2['Dataset']='CIFAR-10'\n",
        "c_metrics_2['Model name']='AlexNet'\n",
        "metrics_c.append(c_metrics_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUfXaRzwHWQO",
        "outputId": "6e7c257c-d4c3-417e-8cd7-026226e691f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 2.3035\n",
            "Epoch [2/20], Loss: 2.3030\n",
            "Epoch [3/20], Loss: 2.3030\n",
            "Epoch [4/20], Loss: 2.3028\n",
            "Epoch [5/20], Loss: 2.3028\n",
            "Epoch [6/20], Loss: 2.3028\n",
            "Epoch [7/20], Loss: 2.3028\n",
            "Epoch [8/20], Loss: 2.3029\n",
            "Epoch [9/20], Loss: 2.3027\n",
            "Epoch [10/20], Loss: 2.3027\n",
            "Epoch [11/20], Loss: 2.3027\n",
            "Epoch [12/20], Loss: 2.3027\n",
            "Epoch [13/20], Loss: 2.3027\n",
            "Epoch [14/20], Loss: 2.3027\n",
            "Epoch [15/20], Loss: 2.3027\n",
            "Epoch [16/20], Loss: 2.3027\n",
            "Epoch [17/20], Loss: 2.3027\n",
            "Epoch [18/20], Loss: 2.3027\n",
            "Epoch [19/20], Loss: 2.3028\n",
            "Epoch [20/20], Loss: 2.3027\n",
            "\n",
            "Accuracy: 10.00%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_3 = VGG(num_classes=10).to(device)\n",
        "c_train_loss_list_3, c_metrics_3 = training_and_evaluating(model_3, train_loader, test_loader, 20)\n",
        "c_metrics_3['Dataset']='CIFAR-10'\n",
        "c_metrics_3['Model name']='VGG'\n",
        "metrics_c.append(c_metrics_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uncjhp6DHWQO",
        "outputId": "e1e1dd42-3296-42da-c0cf-c877ef97a167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.3957\n",
            "Epoch [2/20], Loss: 0.9884\n",
            "Epoch [3/20], Loss: 0.8179\n",
            "Epoch [4/20], Loss: 0.6950\n",
            "Epoch [5/20], Loss: 0.6010\n",
            "Epoch [6/20], Loss: 0.5088\n",
            "Epoch [7/20], Loss: 0.4276\n",
            "Epoch [8/20], Loss: 0.3528\n",
            "Epoch [9/20], Loss: 0.2843\n",
            "Epoch [10/20], Loss: 0.2251\n",
            "Epoch [11/20], Loss: 0.1866\n",
            "Epoch [12/20], Loss: 0.1548\n",
            "Epoch [13/20], Loss: 0.1311\n",
            "Epoch [14/20], Loss: 0.1121\n",
            "Epoch [15/20], Loss: 0.0986\n",
            "Epoch [16/20], Loss: 0.0953\n",
            "Epoch [17/20], Loss: 0.1039\n",
            "Epoch [18/20], Loss: 0.0719\n",
            "Epoch [19/20], Loss: 0.0687\n",
            "Epoch [20/20], Loss: 0.0703\n",
            "\n",
            "Accuracy: 77.18%\n",
            "Precision: 0.78\n",
            "Recall: 0.78\n",
            "F1-score: 0.78\n"
          ]
        }
      ],
      "source": [
        "model_4 = ResNet18(num_classes=10).to(device)\n",
        "c_train_loss_list_4, c_metrics_4= training_and_evaluating(model_4, train_loader, test_loader, 20)\n",
        "c_metrics_4['Dataset']='CIFAR-10'\n",
        "c_metrics_4['Model name']='ResNet18'\n",
        "metrics_c.append(c_metrics_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEsbXwSiHWQP",
        "outputId": "74b66632-7b9d-4cb2-85c6-cf164c54c026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.3110\n",
            "Epoch [2/20], Loss: 0.9305\n",
            "Epoch [3/20], Loss: 0.7626\n",
            "Epoch [4/20], Loss: 0.6413\n",
            "Epoch [5/20], Loss: 0.5363\n",
            "Epoch [6/20], Loss: 0.4418\n",
            "Epoch [7/20], Loss: 0.3493\n",
            "Epoch [8/20], Loss: 0.2812\n",
            "Epoch [9/20], Loss: 0.2175\n",
            "Epoch [10/20], Loss: 0.1781\n",
            "Epoch [11/20], Loss: 0.1452\n",
            "Epoch [12/20], Loss: 0.1276\n",
            "Epoch [13/20], Loss: 0.1111\n",
            "Epoch [14/20], Loss: 0.0924\n",
            "Epoch [15/20], Loss: 0.0905\n",
            "Epoch [16/20], Loss: 0.0840\n",
            "Epoch [17/20], Loss: 0.0822\n",
            "Epoch [18/20], Loss: 0.0737\n",
            "Epoch [19/20], Loss: 0.0647\n",
            "Epoch [20/20], Loss: 0.0617\n",
            "\n",
            "Accuracy: 76.20%\n",
            "Precision: 0.78\n",
            "Recall: 0.78\n",
            "F1-score: 0.78\n"
          ]
        }
      ],
      "source": [
        "model_5 = SENet18(num_classes=10).to(device)\n",
        "c_train_loss_list_5, c_metrics_5 = training_and_evaluating(model_5, train_loader, test_loader, 20)\n",
        "c_metrics_5['Dataset']='CIFAR-10'\n",
        "c_metrics_5['Model name']='SENet18'\n",
        "metrics_c.append(c_metrics_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sa0GKsbHWQP",
        "outputId": "93fecbe7-5c6b-4f4e-e607-8d5a9eb46738"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.6047\n",
            "Epoch [2/20], Loss: 1.1880\n",
            "Epoch [3/20], Loss: 1.0358\n",
            "Epoch [4/20], Loss: 0.9572\n",
            "Epoch [5/20], Loss: 0.9048\n",
            "Epoch [6/20], Loss: 0.8734\n",
            "Epoch [7/20], Loss: 0.8426\n",
            "Epoch [8/20], Loss: 0.8305\n",
            "Epoch [9/20], Loss: 0.8068\n",
            "Epoch [10/20], Loss: 0.8076\n",
            "Epoch [11/20], Loss: 0.8184\n",
            "Epoch [12/20], Loss: 0.7802\n",
            "Epoch [13/20], Loss: 0.7738\n",
            "Epoch [14/20], Loss: 0.7793\n",
            "Epoch [15/20], Loss: 0.8117\n",
            "Epoch [16/20], Loss: 0.8034\n",
            "Epoch [17/20], Loss: 0.7954\n",
            "Epoch [18/20], Loss: 0.7959\n",
            "Epoch [19/20], Loss: 0.7859\n",
            "Epoch [20/20], Loss: 0.8345\n",
            "\n",
            "Accuracy: 10.00%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_6 = GoogleNet(num_classes=10).to(device)\n",
        "c_train_loss_list_6, c_metrics_6 = training_and_evaluating(model_6, train_loader, test_loader, 20)\n",
        "c_metrics_6['Dataset']='CIFAR-10'\n",
        "c_metrics_6['Model name']='GoogleNet'\n",
        "metrics_c.append(c_metrics_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGzEAEXLjiDc"
      },
      "source": [
        "**Creating CSV File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QdSU4N-8Nb2V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "c_loss = {}\n",
        "c_loss['LeNet5'] = c_train_loss_list_1\n",
        "c_loss['AlexNet'] = c_train_loss_list_2\n",
        "c_loss['VGG'] = c_train_loss_list_3\n",
        "c_loss['ResNet18'] = c_train_loss_list_4\n",
        "c_loss['SENet18'] = c_train_loss_list_5\n",
        "c_loss['GoogleNet'] = c_train_loss_list_6\n",
        "\n",
        "df_c_loss = pd.DataFrame(c_loss)\n",
        "df_c_loss.to_csv(\"cifar_10_loss.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9Qi7BuVuBeGN"
      },
      "outputs": [],
      "source": [
        "df_c_metrics = pd.DataFrame(metrics_c)\n",
        "df_c_metrics.to_csv(\"cifar_10_metrics.csv\",index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
