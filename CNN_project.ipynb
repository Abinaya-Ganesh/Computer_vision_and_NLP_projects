{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7eGy_xMoUHxr"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCr0VcdLfXFw"
      },
      "source": [
        "LeNet5, AlexNet, VGG, ResNet18, SENet18 and GoogleNet Architrctures are defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNY9GvjJDko5"
      },
      "source": [
        "**LeNet5 ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLX1kiFU13FO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwEAdnHDrjw"
      },
      "source": [
        "**AlexNet ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87DGAxLTCSnh"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8M638EgDvc1"
      },
      "source": [
        "**VGG ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MuJbk66DguaO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogTm6rQ_EQBF"
      },
      "source": [
        "**ResNet18 ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQbwBqMF2Xqt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride):\n",
        "        strides = [stride] + [1]*(blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, (1,1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-uTN-zkLt85"
      },
      "source": [
        "**SENet18 ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku79cSXsCvR3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
        "        y = F.relu(self.fc1(y))\n",
        "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.se = SEBlock(planes, reduction)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class SENet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(SENet18, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(SEBasicBlock, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(SEBasicBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(SEBasicBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(SEBasicBlock, 512, 2, stride=2)\n",
        "        self.linear = nn.Linear(512 * SEBasicBlock.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIZoq3o0ECfL"
      },
      "source": [
        "**GoogleNet ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "es5cLvv1CwYK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
        "        super(Inception, self).__init__()\n",
        "        self.branch1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
        "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
        "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        outputs = branch1, branch2, branch3, branch4\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "class GoogleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3, aux_logits=True):\n",
        "        super(GoogleNet, self).__init__()\n",
        "        self.aux_logits = aux_logits\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        if aux_logits:\n",
        "            self.aux1 = InceptionAux(512, num_classes)\n",
        "            self.aux2 = InceptionAux(528, num_classes)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.inception4a(x)\n",
        "        if self.aux_logits and self.training:\n",
        "            aux1 = self.aux1(x)\n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        if self.aux_logits and self.training:\n",
        "            aux2 = self.aux2(x)\n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            return aux1, aux2, x\n",
        "        return x\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(InceptionAux, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(128*4*4, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x), inplace=True)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds_xKhxKB3O0"
      },
      "source": [
        "**DOWNLOADING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JoHtzqH2nZoz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_datasets():\n",
        "    transform_mnist = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    transform_cifar = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "\n",
        "    fmnist_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "    fmnist_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "\n",
        "    cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
        "    cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "\n",
        "    return {\n",
        "        'MNIST': (mnist_train, mnist_test),\n",
        "        'FMNIST': (fmnist_train, fmnist_test),\n",
        "        'CIFAR-10': (cifar10_train, cifar10_test)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXVB20qyyrdL",
        "outputId": "b9351f2d-6ba8-4d85-d0c6-fa6490d9de88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4525360.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 56913.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1076482.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4632910.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 10449857.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 166197.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:02<00:00, 1726050.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6224351.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:18<00:00, 9098514.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Function call for getting the data\n",
        "data = get_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sxQeGkp1iDN",
        "outputId": "796b217d-0306-4404-8df4-2480753eebe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tipMoHicB6_Z"
      },
      "source": [
        "**USER DEFINED FUNCTION FOR TRAINING AND EVALUATING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0BDMjN4-Arq6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def training_and_evaluating(model, train_loader, test_loader, num_epochs):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "  train_loss_list = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    #getting the loss curve\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
        "\n",
        "  # Test the model\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  #calculating accuracy\n",
        "  accuracy = 100 * correct / total\n",
        "\n",
        "  #other evaluation metrics\n",
        "  precision = precision_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "  recall = recall_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "  f1 = f1_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "\n",
        "  print(f'\\nAccuracy: {accuracy:.2f}%')\n",
        "  print(f'Precision: {precision:.2f}')\n",
        "  print(f'Recall: {recall:.2f}')\n",
        "  print(f'F1-score: {f1:.2f}')\n",
        "\n",
        "  metrics = {'Accuracy':accuracy, 'Precision':precision, 'Recall':recall, 'F1-score':f1 }\n",
        "\n",
        "  return train_loss_list, metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training and evaluating GoogleNet\n",
        "#weighted loss is calculated for original and auxiliary outputs\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def training_and_evaluating_GoogleNet(model, train_loader, test_loader, num_epochs):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "  train_loss_list = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      aux1, aux2, outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      aux_loss_1 = criterion(aux1, labels)\n",
        "      aux_loss_2 = criterion(aux2, labels)\n",
        "      total_loss = loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += total_loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    #getting the loss curve\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
        "\n",
        "  # Test the model\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  #calculating accuracy\n",
        "  accuracy = 100 * correct / total\n",
        "\n",
        "  #other evaluation metrics\n",
        "  precision = precision_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "  recall = recall_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "  f1 = f1_score(labels.cpu(), predicted.cpu(),average='macro')\n",
        "\n",
        "  print(f'\\nAccuracy: {accuracy:.2f}%')\n",
        "  print(f'Precision: {precision:.2f}')\n",
        "  print(f'Recall: {recall:.2f}')\n",
        "  print(f'F1-score: {f1:.2f}')\n",
        "\n",
        "  metrics = {'Accuracy':accuracy, 'Precision':precision, 'Recall':recall, 'F1-score':f1 }\n",
        "\n",
        "  return train_loss_list, metrics"
      ],
      "metadata": {
        "id": "WJzJ9vnpU6mi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DdbPa4tBceL"
      },
      "source": [
        "**MNIST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zRyttVYAuA1C"
      },
      "outputs": [],
      "source": [
        "#Data_loader for MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['MNIST'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['MNIST'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VdcZatPizlp"
      },
      "source": [
        "**Function call for each model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t89XxG7RBjn9",
        "outputId": "f64e2992-96bd-4e2b-eefe-42826a6af5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.1955\n",
            "Epoch [2/20], Loss: 0.0504\n",
            "Epoch [3/20], Loss: 0.0337\n",
            "Epoch [4/20], Loss: 0.0248\n",
            "Epoch [5/20], Loss: 0.0186\n",
            "Epoch [6/20], Loss: 0.0146\n",
            "Epoch [7/20], Loss: 0.0118\n",
            "Epoch [8/20], Loss: 0.0098\n",
            "Epoch [9/20], Loss: 0.0086\n",
            "Epoch [10/20], Loss: 0.0077\n",
            "Epoch [11/20], Loss: 0.0070\n",
            "Epoch [12/20], Loss: 0.0066\n",
            "Epoch [13/20], Loss: 0.0063\n",
            "Epoch [14/20], Loss: 0.0062\n",
            "Epoch [15/20], Loss: 0.0060\n",
            "Epoch [16/20], Loss: 0.0059\n",
            "Epoch [17/20], Loss: 0.0058\n",
            "Epoch [18/20], Loss: 0.0058\n",
            "Epoch [19/20], Loss: 0.0057\n",
            "Epoch [20/20], Loss: 0.0057\n",
            "\n",
            "Accuracy: 99.25%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ],
      "source": [
        "#parameters of training_and_evaluating fumction --> model, train_loader, test_loader, num_epochs\n",
        "\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "m_train_loss_list_1, m_metrics_1 = training_and_evaluating(model_1, train_loader, test_loader, 20)\n",
        "m_metrics_1['Dataset']='MNIST'\n",
        "m_metrics_1['Model name']='LeNet5'\n",
        "pickle.dump(m_train_loss_list_1, open(\"m_train_loss_list_1.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_1, open(\"m_metrics_1.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXg5kOgbRwqO",
        "outputId": "791b46ee-9bc8-4c5c-feff-8bae2364a462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4093\n",
            "Epoch [2/20], Loss: 0.0884\n",
            "Epoch [3/20], Loss: 0.0474\n",
            "Epoch [4/20], Loss: 0.0295\n",
            "Epoch [5/20], Loss: 0.0156\n",
            "Epoch [6/20], Loss: 0.0095\n",
            "Epoch [7/20], Loss: 0.0048\n",
            "Epoch [8/20], Loss: 0.0033\n",
            "Epoch [9/20], Loss: 0.0017\n",
            "Epoch [10/20], Loss: 0.0008\n",
            "Epoch [11/20], Loss: 0.0005\n",
            "Epoch [12/20], Loss: 0.0004\n",
            "Epoch [13/20], Loss: 0.0003\n",
            "Epoch [14/20], Loss: 0.0002\n",
            "Epoch [15/20], Loss: 0.0002\n",
            "Epoch [16/20], Loss: 0.0003\n",
            "Epoch [17/20], Loss: 0.0001\n",
            "Epoch [18/20], Loss: 0.0001\n",
            "Epoch [19/20], Loss: 0.0001\n",
            "Epoch [20/20], Loss: 0.0001\n",
            "\n",
            "Accuracy: 99.29%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ],
      "source": [
        "model_2 = AlexNet(num_classes=10).to(device)\n",
        "m_train_loss_list_2, m_metrics_2 = training_and_evaluating(model_2, train_loader, test_loader, 20)\n",
        "m_metrics_2['Dataset']='MNIST'\n",
        "m_metrics_2['Model name']='AlexNet'\n",
        "pickle.dump(m_train_loss_list_2, open(\"m_train_loss_list_2.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_2, open(\"m_metrics_2.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7JnH2Y3hCsK",
        "outputId": "fe6fdc76-da3c-46c1-9006-ace3a3d28892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.3021\n",
            "Epoch [2/20], Loss: 2.3015\n",
            "Epoch [3/20], Loss: 2.3013\n",
            "Epoch [4/20], Loss: 2.3013\n",
            "Epoch [5/20], Loss: 2.3013\n",
            "Epoch [6/20], Loss: 2.3012\n",
            "Epoch [7/20], Loss: 2.3012\n",
            "Epoch [8/20], Loss: 2.3012\n",
            "Epoch [9/20], Loss: 2.3012\n",
            "Epoch [10/20], Loss: 2.3012\n",
            "Epoch [11/20], Loss: 2.3012\n",
            "Epoch [12/20], Loss: 2.3012\n",
            "Epoch [13/20], Loss: 2.3012\n",
            "Epoch [14/20], Loss: 2.3012\n",
            "Epoch [15/20], Loss: 2.3012\n",
            "Epoch [16/20], Loss: 2.3012\n",
            "Epoch [17/20], Loss: 2.3012\n",
            "Epoch [18/20], Loss: 2.3012\n",
            "Epoch [19/20], Loss: 2.3012\n",
            "Epoch [20/20], Loss: 2.3012\n",
            "\n",
            "Accuracy: 11.35%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_3 = VGG(num_classes=10).to(device)\n",
        "m_train_loss_list_3, m_metrics_3 = training_and_evaluating(model_3, train_loader, test_loader, 20)\n",
        "m_metrics_3['Dataset']='MNIST'\n",
        "m_metrics_3['Model name']='VGG'\n",
        "pickle.dump(m_train_loss_list_3, open(\"m_train_loss_list_3.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_3, open(\"m_metrics_3.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-CwdNO0Al7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907e326f-3491-45b3-8ce5-8cfe0d0d006e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.1344\n",
            "Epoch [2/20], Loss: 0.0513\n",
            "Epoch [3/20], Loss: 0.0333\n",
            "Epoch [4/20], Loss: 0.0222\n",
            "Epoch [5/20], Loss: 0.0162\n",
            "Epoch [6/20], Loss: 0.0110\n",
            "Epoch [7/20], Loss: 0.0072\n",
            "Epoch [8/20], Loss: 0.0050\n",
            "Epoch [9/20], Loss: 0.0034\n",
            "Epoch [10/20], Loss: 0.0020\n",
            "Epoch [11/20], Loss: 0.0014\n",
            "Epoch [12/20], Loss: 0.0011\n",
            "Epoch [13/20], Loss: 0.0008\n",
            "Epoch [14/20], Loss: 0.0008\n",
            "Epoch [15/20], Loss: 0.0005\n",
            "Epoch [16/20], Loss: 0.0004\n",
            "Epoch [17/20], Loss: 0.0004\n",
            "Epoch [18/20], Loss: 0.0003\n",
            "Epoch [19/20], Loss: 0.0003\n",
            "Epoch [20/20], Loss: 0.0003\n",
            "\n",
            "Accuracy: 99.50%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ],
      "source": [
        "model_4 = ResNet18(num_classes=10).to(device)\n",
        "m_train_loss_list_4, m_metrics_4 = training_and_evaluating(model_4, train_loader, test_loader, 20)\n",
        "m_metrics_4['Dataset']='MNIST'\n",
        "m_metrics_4['Model name']='ResNet18'\n",
        "pickle.dump(m_train_loss_list_4, open(\"m_train_loss_list_4.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_4, open(\"m_metrics_4.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUenEqE7AmOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdbfe40-3b6f-4a40-972f-8b083b3fe66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.1150\n",
            "Epoch [2/20], Loss: 0.0440\n",
            "Epoch [3/20], Loss: 0.0294\n",
            "Epoch [4/20], Loss: 0.0206\n",
            "Epoch [5/20], Loss: 0.0138\n",
            "Epoch [6/20], Loss: 0.0095\n",
            "Epoch [7/20], Loss: 0.0062\n",
            "Epoch [8/20], Loss: 0.0039\n",
            "Epoch [9/20], Loss: 0.0029\n",
            "Epoch [10/20], Loss: 0.0018\n",
            "Epoch [11/20], Loss: 0.0012\n",
            "Epoch [12/20], Loss: 0.0008\n",
            "Epoch [13/20], Loss: 0.0007\n",
            "Epoch [14/20], Loss: 0.0006\n",
            "Epoch [15/20], Loss: 0.0005\n",
            "Epoch [16/20], Loss: 0.0004\n",
            "Epoch [17/20], Loss: 0.0004\n",
            "Epoch [18/20], Loss: 0.0003\n",
            "Epoch [19/20], Loss: 0.0003\n",
            "Epoch [20/20], Loss: 0.0003\n",
            "\n",
            "Accuracy: 99.45%\n",
            "Precision: 1.00\n",
            "Recall: 0.99\n",
            "F1-score: 1.00\n"
          ]
        }
      ],
      "source": [
        "model_5 = SENet18(num_classes=10).to(device)\n",
        "m_train_loss_list_5, m_metrics_5 = training_and_evaluating(model_5, train_loader, test_loader, 20)\n",
        "m_metrics_5['Dataset']='MNIST'\n",
        "m_metrics_5['Model name']='SENet18'\n",
        "pickle.dump(m_train_loss_list_5, open(\"m_train_loss_list_5.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_5, open(\"m_metrics_5.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6 = GoogleNet(num_classes=10).to(device)\n",
        "m_train_loss_list_6, m_metrics_6 = training_and_evaluating_GoogleNet(model_6, train_loader, test_loader, 20)\n",
        "m_metrics_6['Dataset']='MNIST'\n",
        "m_metrics_6['Model name']='GoogleNet'\n",
        "pickle.dump(m_train_loss_list_6, open(\"m_train_loss_list_6.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_6, open(\"m_metrics_6.p\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufbgJHIJbcJE",
        "outputId": "7e82e417-7ffc-496c-f6be-e4b8e1ebab07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4807\n",
            "Epoch [2/20], Loss: 0.1030\n",
            "Epoch [3/20], Loss: 0.0628\n",
            "Epoch [4/20], Loss: 0.0376\n",
            "Epoch [5/20], Loss: 0.0223\n",
            "Epoch [6/20], Loss: 0.0143\n",
            "Epoch [7/20], Loss: 0.0058\n",
            "Epoch [8/20], Loss: 0.0031\n",
            "Epoch [9/20], Loss: 0.0008\n",
            "Epoch [10/20], Loss: 0.0001\n",
            "Epoch [11/20], Loss: 0.0001\n",
            "Epoch [12/20], Loss: 0.0000\n",
            "Epoch [13/20], Loss: 0.0000\n",
            "Epoch [14/20], Loss: 0.0000\n",
            "Epoch [15/20], Loss: 0.0000\n",
            "Epoch [16/20], Loss: 0.0000\n",
            "Epoch [17/20], Loss: 0.0000\n",
            "Epoch [18/20], Loss: 0.0000\n",
            "Epoch [19/20], Loss: 0.0000\n",
            "Epoch [20/20], Loss: 0.0000\n",
            "\n",
            "Accuracy: 99.55%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZbUT0nwG2aM"
      },
      "source": [
        "**FASHION MNIST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifoCJ792F4Po"
      },
      "outputs": [],
      "source": [
        "#Data_loader for FASHION MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['FMNIST'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['FMNIST'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNUKo8jjMDJ"
      },
      "source": [
        "**Function call for each model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_t9RHQ7G0U3",
        "outputId": "75b10b2c-623e-4574-c2d6-b13bee81c2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.5674\n",
            "Epoch [2/20], Loss: 0.3654\n",
            "Epoch [3/20], Loss: 0.3168\n",
            "Epoch [4/20], Loss: 0.2900\n",
            "Epoch [5/20], Loss: 0.2699\n",
            "Epoch [6/20], Loss: 0.2578\n",
            "Epoch [7/20], Loss: 0.2470\n",
            "Epoch [8/20], Loss: 0.2403\n",
            "Epoch [9/20], Loss: 0.2350\n",
            "Epoch [10/20], Loss: 0.2314\n",
            "Epoch [11/20], Loss: 0.2287\n",
            "Epoch [12/20], Loss: 0.2272\n",
            "Epoch [13/20], Loss: 0.2258\n",
            "Epoch [14/20], Loss: 0.2249\n",
            "Epoch [15/20], Loss: 0.2242\n",
            "Epoch [16/20], Loss: 0.2238\n",
            "Epoch [17/20], Loss: 0.2235\n",
            "Epoch [18/20], Loss: 0.2232\n",
            "Epoch [19/20], Loss: 0.2230\n",
            "Epoch [20/20], Loss: 0.2230\n",
            "\n",
            "Accuracy: 89.72%\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-score: 0.89\n"
          ]
        }
      ],
      "source": [
        "#parameters of training_and_evaluating fumction --> model, train_loader, test_loader, num_epochs\n",
        "\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "f_train_loss_list_1, f_metrics_1 = training_and_evaluating(model_1, train_loader, test_loader, 20)\n",
        "f_metrics_1['Dataset']='FASHION MNIST'\n",
        "f_metrics_1['Model name']='LeNet5'\n",
        "pickle.dump(f_train_loss_list_1, open(\"f_train_loss_list_1.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_1, open(\"f_metrics_1.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To3Edb85G0U7",
        "outputId": "3e8e2b69-66cf-47e2-cbc1-5492127b0095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.7550\n",
            "Epoch [2/20], Loss: 0.4043\n",
            "Epoch [3/20], Loss: 0.3343\n",
            "Epoch [4/20], Loss: 0.2874\n",
            "Epoch [5/20], Loss: 0.2516\n",
            "Epoch [6/20], Loss: 0.2206\n",
            "Epoch [7/20], Loss: 0.1936\n",
            "Epoch [8/20], Loss: 0.1700\n",
            "Epoch [9/20], Loss: 0.1537\n",
            "Epoch [10/20], Loss: 0.1384\n",
            "Epoch [11/20], Loss: 0.1280\n",
            "Epoch [12/20], Loss: 0.1206\n",
            "Epoch [13/20], Loss: 0.1141\n",
            "Epoch [14/20], Loss: 0.1103\n",
            "Epoch [15/20], Loss: 0.1078\n",
            "Epoch [16/20], Loss: 0.1055\n",
            "Epoch [17/20], Loss: 0.1040\n",
            "Epoch [18/20], Loss: 0.1040\n",
            "Epoch [19/20], Loss: 0.1019\n",
            "Epoch [20/20], Loss: 0.1021\n",
            "\n",
            "Accuracy: 89.88%\n",
            "Precision: 0.90\n",
            "Recall: 0.91\n",
            "F1-score: 0.90\n"
          ]
        }
      ],
      "source": [
        "model_2 = AlexNet(num_classes=10).to(device)\n",
        "f_train_loss_list_2, f_metrics_2 = training_and_evaluating(model_2, train_loader, test_loader, 20)\n",
        "f_metrics_2['Dataset']='FASHION MNIST'\n",
        "f_metrics_2['Model name']='AlexNet'\n",
        "pickle.dump(f_train_loss_list_2, open(\"f_train_loss_list_2.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_2, open(\"f_metrics_2.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaBNM6y7G0U7",
        "outputId": "6810cca7-112a-42aa-a1f0-3c37c77ae983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.3084\n",
            "Epoch [2/20], Loss: 2.3028\n",
            "Epoch [3/20], Loss: 2.3027\n",
            "Epoch [4/20], Loss: 2.3027\n",
            "Epoch [5/20], Loss: 2.3027\n",
            "Epoch [6/20], Loss: 2.3026\n",
            "Epoch [7/20], Loss: 2.3026\n",
            "Epoch [8/20], Loss: 2.3026\n",
            "Epoch [9/20], Loss: 2.3026\n",
            "Epoch [10/20], Loss: 2.3026\n",
            "Epoch [11/20], Loss: 2.3026\n",
            "Epoch [12/20], Loss: 2.3026\n",
            "Epoch [13/20], Loss: 2.3026\n",
            "Epoch [14/20], Loss: 2.3026\n",
            "Epoch [15/20], Loss: 2.3026\n",
            "Epoch [16/20], Loss: 2.3026\n",
            "Epoch [17/20], Loss: 2.3026\n",
            "Epoch [18/20], Loss: 2.3026\n",
            "Epoch [19/20], Loss: 2.3026\n",
            "Epoch [20/20], Loss: 2.3026\n",
            "\n",
            "Accuracy: 10.00%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_3 = VGG(num_classes=10).to(device)\n",
        "f_train_loss_list_3, f_metrics_3 = training_and_evaluating(model_3, train_loader, test_loader, 20)\n",
        "f_metrics_3['Dataset']='FASHION MNIST'\n",
        "f_metrics_3['Model name']='VGG'\n",
        "pickle.dump(f_train_loss_list_3, open(\"f_train_loss_list_3.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_3, open(\"f_metrics_3.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcVpm0kYG0U8",
        "outputId": "95935149-bab6-499e-a76b-31b54b64d301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4332\n",
            "Epoch [2/20], Loss: 0.2959\n",
            "Epoch [3/20], Loss: 0.2452\n",
            "Epoch [4/20], Loss: 0.2089\n",
            "Epoch [5/20], Loss: 0.1776\n",
            "Epoch [6/20], Loss: 0.1478\n",
            "Epoch [7/20], Loss: 0.1178\n",
            "Epoch [8/20], Loss: 0.0937\n",
            "Epoch [9/20], Loss: 0.0710\n",
            "Epoch [10/20], Loss: 0.0550\n",
            "Epoch [11/20], Loss: 0.0416\n",
            "Epoch [12/20], Loss: 0.0324\n",
            "Epoch [13/20], Loss: 0.0255\n",
            "Epoch [14/20], Loss: 0.0202\n",
            "Epoch [15/20], Loss: 0.0182\n",
            "Epoch [16/20], Loss: 0.0155\n",
            "Epoch [17/20], Loss: 0.0137\n",
            "Epoch [18/20], Loss: 0.0127\n",
            "Epoch [19/20], Loss: 0.0125\n",
            "Epoch [20/20], Loss: 0.0122\n",
            "\n",
            "Accuracy: 92.60%\n",
            "Precision: 0.92\n",
            "Recall: 0.93\n",
            "F1-score: 0.92\n"
          ]
        }
      ],
      "source": [
        "model_4 = ResNet18(num_classes=10).to(device)\n",
        "f_train_loss_list_4, f_metrics_4 = training_and_evaluating(model_4, train_loader, test_loader, 20)\n",
        "f_metrics_4['Dataset']='FASHION MNIST'\n",
        "f_metrics_4['Model name']='ResNet18'\n",
        "pickle.dump(f_train_loss_list_4, open(\"f_train_loss_list_4.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_4, open(\"f_metrics_4.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5woM-jBG0U9",
        "outputId": "7a28c83e-2174-4706-ee91-b3c44ff9e381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4144\n",
            "Epoch [2/20], Loss: 0.2777\n",
            "Epoch [3/20], Loss: 0.2303\n",
            "Epoch [4/20], Loss: 0.1927\n",
            "Epoch [5/20], Loss: 0.1588\n",
            "Epoch [6/20], Loss: 0.1279\n",
            "Epoch [7/20], Loss: 0.0999\n",
            "Epoch [8/20], Loss: 0.0772\n",
            "Epoch [9/20], Loss: 0.0576\n",
            "Epoch [10/20], Loss: 0.0448\n",
            "Epoch [11/20], Loss: 0.0351\n",
            "Epoch [12/20], Loss: 0.0276\n",
            "Epoch [13/20], Loss: 0.0214\n",
            "Epoch [14/20], Loss: 0.0187\n",
            "Epoch [15/20], Loss: 0.0159\n",
            "Epoch [16/20], Loss: 0.0147\n",
            "Epoch [17/20], Loss: 0.0135\n",
            "Epoch [18/20], Loss: 0.0126\n",
            "Epoch [19/20], Loss: 0.0120\n",
            "Epoch [20/20], Loss: 0.0117\n",
            "\n",
            "Accuracy: 92.15%\n",
            "Precision: 0.93\n",
            "Recall: 0.93\n",
            "F1-score: 0.93\n"
          ]
        }
      ],
      "source": [
        "model_5 = SENet18(num_classes=10).to(device)\n",
        "f_train_loss_list_5, f_metrics_5 = training_and_evaluating(model_5, train_loader, test_loader, 20)\n",
        "f_metrics_5['Dataset']='FASHION MNIST'\n",
        "f_metrics_5['Model name']='SENet18'\n",
        "pickle.dump(f_train_loss_list_5, open(\"f_train_loss_list_5.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_5, open(\"f_metrics_5.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6 = GoogleNet(num_classes=10).to(device)\n",
        "f_train_loss_list_6, f_metrics_6 = training_and_evaluating_GoogleNet(model_6, train_loader, test_loader, 20)\n",
        "f_metrics_6['Dataset']='FASHION MNIST'\n",
        "f_metrics_6['Model name']='GoogleNet'\n",
        "pickle.dump(f_train_loss_list_6, open(\"f_train_loss_list_6.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_6, open(\"f_metrics_6.p\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06dmLX1MOoVo",
        "outputId": "d5bc0b71-07a5-488d-82bc-a58ca6c15299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.0181\n",
            "Epoch [2/20], Loss: 0.5377\n",
            "Epoch [3/20], Loss: 0.4240\n",
            "Epoch [4/20], Loss: 0.3400\n",
            "Epoch [5/20], Loss: 0.2625\n",
            "Epoch [6/20], Loss: 0.1923\n",
            "Epoch [7/20], Loss: 0.1295\n",
            "Epoch [8/20], Loss: 0.0788\n",
            "Epoch [9/20], Loss: 0.0415\n",
            "Epoch [10/20], Loss: 0.0183\n",
            "Epoch [11/20], Loss: 0.0086\n",
            "Epoch [12/20], Loss: 0.0033\n",
            "Epoch [13/20], Loss: 0.0015\n",
            "Epoch [14/20], Loss: 0.0009\n",
            "Epoch [15/20], Loss: 0.0008\n",
            "Epoch [16/20], Loss: 0.0006\n",
            "Epoch [17/20], Loss: 0.0005\n",
            "Epoch [18/20], Loss: 0.0004\n",
            "Epoch [19/20], Loss: 0.0003\n",
            "Epoch [20/20], Loss: 0.0003\n",
            "\n",
            "Accuracy: 91.24%\n",
            "Precision: 0.92\n",
            "Recall: 0.92\n",
            "F1-score: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqMUBnkQHKXA"
      },
      "source": [
        "**CIFAR-10 DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OgXTjyJaHWQK"
      },
      "outputs": [],
      "source": [
        "#Data_loader for CIFAR-10 DATA\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['CIFAR-10'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['CIFAR-10'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCds1xvqjOjG"
      },
      "source": [
        "**Function call for each model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_ULpVGhHWQL",
        "outputId": "bfef51f1-627e-4ddc-ada8-b7ab6dc8ea96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.6264\n",
            "Epoch [2/100], Loss: 1.3515\n",
            "Epoch [3/100], Loss: 1.2498\n",
            "Epoch [4/100], Loss: 1.1845\n",
            "Epoch [5/100], Loss: 1.1404\n",
            "Epoch [6/100], Loss: 1.1083\n",
            "Epoch [7/100], Loss: 1.0871\n",
            "Epoch [8/100], Loss: 1.0720\n",
            "Epoch [9/100], Loss: 1.0605\n",
            "Epoch [10/100], Loss: 1.0525\n",
            "Epoch [11/100], Loss: 1.0477\n",
            "Epoch [12/100], Loss: 1.0438\n",
            "Epoch [13/100], Loss: 1.0412\n",
            "Epoch [14/100], Loss: 1.0390\n",
            "Epoch [15/100], Loss: 1.0375\n",
            "Epoch [16/100], Loss: 1.0364\n",
            "Epoch [17/100], Loss: 1.0360\n",
            "Epoch [18/100], Loss: 1.0352\n",
            "Epoch [19/100], Loss: 1.0348\n",
            "Epoch [20/100], Loss: 1.0354\n",
            "Epoch [21/100], Loss: 1.0348\n",
            "Epoch [22/100], Loss: 1.0345\n",
            "Epoch [23/100], Loss: 1.0347\n",
            "Epoch [24/100], Loss: 1.0346\n",
            "Epoch [25/100], Loss: 1.0344\n",
            "Epoch [26/100], Loss: 1.0343\n",
            "Epoch [27/100], Loss: 1.0342\n",
            "Epoch [28/100], Loss: 1.0341\n",
            "Epoch [29/100], Loss: 1.0346\n",
            "Epoch [30/100], Loss: 1.0345\n",
            "Epoch [31/100], Loss: 1.0341\n",
            "Epoch [32/100], Loss: 1.0348\n",
            "Epoch [33/100], Loss: 1.0344\n",
            "Epoch [34/100], Loss: 1.0346\n",
            "Epoch [35/100], Loss: 1.0344\n",
            "Epoch [36/100], Loss: 1.0345\n",
            "Epoch [37/100], Loss: 1.0347\n",
            "Epoch [38/100], Loss: 1.0341\n",
            "Epoch [39/100], Loss: 1.0343\n",
            "Epoch [40/100], Loss: 1.0343\n",
            "Epoch [41/100], Loss: 1.0343\n",
            "Epoch [42/100], Loss: 1.0341\n",
            "Epoch [43/100], Loss: 1.0342\n",
            "Epoch [44/100], Loss: 1.0347\n",
            "Epoch [45/100], Loss: 1.0343\n",
            "Epoch [46/100], Loss: 1.0341\n",
            "Epoch [47/100], Loss: 1.0341\n",
            "Epoch [48/100], Loss: 1.0342\n",
            "Epoch [49/100], Loss: 1.0344\n",
            "Epoch [50/100], Loss: 1.0342\n",
            "Epoch [51/100], Loss: 1.0342\n",
            "Epoch [52/100], Loss: 1.0345\n",
            "Epoch [53/100], Loss: 1.0343\n",
            "Epoch [54/100], Loss: 1.0342\n",
            "Epoch [55/100], Loss: 1.0342\n",
            "Epoch [56/100], Loss: 1.0342\n",
            "Epoch [57/100], Loss: 1.0345\n",
            "Epoch [58/100], Loss: 1.0341\n",
            "Epoch [59/100], Loss: 1.0341\n",
            "Epoch [60/100], Loss: 1.0342\n",
            "Epoch [61/100], Loss: 1.0342\n",
            "Epoch [62/100], Loss: 1.0350\n",
            "Epoch [63/100], Loss: 1.0348\n",
            "Epoch [64/100], Loss: 1.0346\n",
            "Epoch [65/100], Loss: 1.0339\n",
            "Epoch [66/100], Loss: 1.0343\n",
            "Epoch [67/100], Loss: 1.0346\n",
            "Epoch [68/100], Loss: 1.0347\n",
            "Epoch [69/100], Loss: 1.0338\n",
            "Epoch [70/100], Loss: 1.0344\n",
            "Epoch [71/100], Loss: 1.0348\n",
            "Epoch [72/100], Loss: 1.0344\n",
            "Epoch [73/100], Loss: 1.0345\n",
            "Epoch [74/100], Loss: 1.0343\n",
            "Epoch [75/100], Loss: 1.0348\n",
            "Epoch [76/100], Loss: 1.0342\n",
            "Epoch [77/100], Loss: 1.0345\n",
            "Epoch [78/100], Loss: 1.0345\n",
            "Epoch [79/100], Loss: 1.0344\n",
            "Epoch [80/100], Loss: 1.0348\n",
            "Epoch [81/100], Loss: 1.0339\n",
            "Epoch [82/100], Loss: 1.0343\n",
            "Epoch [83/100], Loss: 1.0348\n",
            "Epoch [84/100], Loss: 1.0344\n",
            "Epoch [85/100], Loss: 1.0343\n",
            "Epoch [86/100], Loss: 1.0343\n",
            "Epoch [87/100], Loss: 1.0345\n",
            "Epoch [88/100], Loss: 1.0343\n",
            "Epoch [89/100], Loss: 1.0341\n",
            "Epoch [90/100], Loss: 1.0343\n",
            "Epoch [91/100], Loss: 1.0344\n",
            "Epoch [92/100], Loss: 1.0346\n",
            "Epoch [93/100], Loss: 1.0346\n",
            "Epoch [94/100], Loss: 1.0345\n",
            "Epoch [95/100], Loss: 1.0343\n",
            "Epoch [96/100], Loss: 1.0339\n",
            "Epoch [97/100], Loss: 1.0341\n",
            "Epoch [98/100], Loss: 1.0342\n",
            "Epoch [99/100], Loss: 1.0343\n",
            "Epoch [100/100], Loss: 1.0345\n",
            "\n",
            "Accuracy: 59.57%\n",
            "Precision: 0.58\n",
            "Recall: 0.58\n",
            "F1-score: 0.58\n"
          ]
        }
      ],
      "source": [
        "#parameters of training_and_evaluating fumction --> model, train_loader, test_loader, num_epochs\n",
        "\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "c_train_loss_list_1, c_metrics_1 = training_and_evaluating(model_1, train_loader, test_loader, 100)\n",
        "c_metrics_1['Dataset']='CIFAR-10'\n",
        "c_metrics_1['Model name']='LeNet5'\n",
        "pickle.dump(c_train_loss_list_1, open(\"c_train_loss_list_1.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_1, open(\"c_metrics_1.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dLBHYd0HWQN",
        "outputId": "ae668ca4-d11b-4cfb-810f-671aa09a1dcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.8890\n",
            "Epoch [2/100], Loss: 1.5239\n",
            "Epoch [3/100], Loss: 1.3117\n",
            "Epoch [4/100], Loss: 1.1658\n",
            "Epoch [5/100], Loss: 1.0356\n",
            "Epoch [6/100], Loss: 0.9273\n",
            "Epoch [7/100], Loss: 0.8302\n",
            "Epoch [8/100], Loss: 0.7452\n",
            "Epoch [9/100], Loss: 0.6752\n",
            "Epoch [10/100], Loss: 0.6229\n",
            "Epoch [11/100], Loss: 0.5862\n",
            "Epoch [12/100], Loss: 0.5580\n",
            "Epoch [13/100], Loss: 0.5381\n",
            "Epoch [14/100], Loss: 0.5248\n",
            "Epoch [15/100], Loss: 0.5142\n",
            "Epoch [16/100], Loss: 0.5083\n",
            "Epoch [17/100], Loss: 0.5033\n",
            "Epoch [18/100], Loss: 0.5004\n",
            "Epoch [19/100], Loss: 0.4988\n",
            "Epoch [20/100], Loss: 0.4951\n",
            "Epoch [21/100], Loss: 0.4935\n",
            "Epoch [22/100], Loss: 0.4937\n",
            "Epoch [23/100], Loss: 0.4944\n",
            "Epoch [24/100], Loss: 0.4935\n",
            "Epoch [25/100], Loss: 0.4918\n",
            "Epoch [26/100], Loss: 0.4928\n",
            "Epoch [27/100], Loss: 0.4930\n",
            "Epoch [28/100], Loss: 0.4937\n",
            "Epoch [29/100], Loss: 0.4914\n",
            "Epoch [30/100], Loss: 0.4915\n",
            "Epoch [31/100], Loss: 0.4925\n",
            "Epoch [32/100], Loss: 0.4929\n",
            "Epoch [33/100], Loss: 0.4920\n",
            "Epoch [34/100], Loss: 0.4920\n",
            "Epoch [35/100], Loss: 0.4928\n",
            "Epoch [36/100], Loss: 0.4932\n",
            "Epoch [37/100], Loss: 0.4914\n",
            "Epoch [38/100], Loss: 0.4902\n",
            "Epoch [39/100], Loss: 0.4917\n",
            "Epoch [40/100], Loss: 0.4903\n",
            "Epoch [41/100], Loss: 0.4913\n",
            "Epoch [42/100], Loss: 0.4920\n",
            "Epoch [43/100], Loss: 0.4923\n",
            "Epoch [44/100], Loss: 0.4927\n",
            "Epoch [45/100], Loss: 0.4920\n",
            "Epoch [46/100], Loss: 0.4927\n",
            "Epoch [47/100], Loss: 0.4916\n",
            "Epoch [48/100], Loss: 0.4904\n",
            "Epoch [49/100], Loss: 0.4917\n",
            "Epoch [50/100], Loss: 0.4913\n",
            "Epoch [51/100], Loss: 0.4946\n",
            "Epoch [52/100], Loss: 0.4923\n",
            "Epoch [53/100], Loss: 0.4904\n",
            "Epoch [54/100], Loss: 0.4937\n",
            "Epoch [55/100], Loss: 0.4926\n",
            "Epoch [56/100], Loss: 0.4929\n",
            "Epoch [57/100], Loss: 0.4928\n",
            "Epoch [58/100], Loss: 0.4931\n",
            "Epoch [59/100], Loss: 0.4930\n",
            "Epoch [60/100], Loss: 0.4902\n",
            "Epoch [61/100], Loss: 0.4911\n",
            "Epoch [62/100], Loss: 0.4937\n",
            "Epoch [63/100], Loss: 0.4914\n",
            "Epoch [64/100], Loss: 0.4934\n",
            "Epoch [65/100], Loss: 0.4926\n",
            "Epoch [66/100], Loss: 0.4922\n",
            "Epoch [67/100], Loss: 0.4905\n",
            "Epoch [68/100], Loss: 0.4931\n",
            "Epoch [69/100], Loss: 0.4909\n",
            "Epoch [70/100], Loss: 0.4936\n",
            "Epoch [71/100], Loss: 0.4919\n",
            "Epoch [72/100], Loss: 0.4914\n",
            "Epoch [73/100], Loss: 0.4948\n",
            "Epoch [74/100], Loss: 0.4903\n",
            "Epoch [75/100], Loss: 0.4922\n",
            "Epoch [76/100], Loss: 0.4916\n",
            "Epoch [77/100], Loss: 0.4920\n",
            "Epoch [78/100], Loss: 0.4926\n",
            "Epoch [79/100], Loss: 0.4923\n",
            "Epoch [80/100], Loss: 0.4917\n",
            "Epoch [81/100], Loss: 0.4917\n",
            "Epoch [82/100], Loss: 0.4927\n",
            "Epoch [83/100], Loss: 0.4946\n",
            "Epoch [84/100], Loss: 0.4907\n",
            "Epoch [85/100], Loss: 0.4903\n",
            "Epoch [86/100], Loss: 0.4914\n",
            "Epoch [87/100], Loss: 0.4912\n",
            "Epoch [88/100], Loss: 0.4915\n",
            "Epoch [89/100], Loss: 0.4920\n",
            "Epoch [90/100], Loss: 0.4923\n",
            "Epoch [91/100], Loss: 0.4911\n",
            "Epoch [92/100], Loss: 0.4913\n",
            "Epoch [93/100], Loss: 0.4928\n",
            "Epoch [94/100], Loss: 0.4908\n",
            "Epoch [95/100], Loss: 0.4929\n",
            "Epoch [96/100], Loss: 0.4922\n",
            "Epoch [97/100], Loss: 0.4894\n",
            "Epoch [98/100], Loss: 0.4919\n",
            "Epoch [99/100], Loss: 0.4906\n",
            "Epoch [100/100], Loss: 0.4932\n",
            "\n",
            "Accuracy: 61.47%\n",
            "Precision: 0.60\n",
            "Recall: 0.60\n",
            "F1-score: 0.60\n"
          ]
        }
      ],
      "source": [
        "model_2 = AlexNet(num_classes=10).to(device)\n",
        "c_train_loss_list_2, c_metrics_2= training_and_evaluating(model_2, train_loader, test_loader, 100)\n",
        "c_metrics_2['Dataset']='CIFAR-10'\n",
        "c_metrics_2['Model name']='AlexNet'\n",
        "pickle.dump(c_train_loss_list_2, open(\"c_train_loss_list_2.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_2, open(\"c_metrics_2.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUfXaRzwHWQO",
        "outputId": "224b8458-f1bd-457c-815a-0bf341ec6016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.3032\n",
            "Epoch [2/20], Loss: 2.3028\n",
            "Epoch [3/20], Loss: 2.3028\n",
            "Epoch [4/20], Loss: 2.3027\n",
            "Epoch [5/20], Loss: 2.3027\n",
            "Epoch [6/20], Loss: 2.3026\n",
            "Epoch [7/20], Loss: 2.3026\n",
            "Epoch [8/20], Loss: 2.3026\n",
            "Epoch [9/20], Loss: 2.3026\n",
            "Epoch [10/20], Loss: 2.3026\n",
            "Epoch [11/20], Loss: 2.3026\n",
            "Epoch [12/20], Loss: 2.3026\n",
            "Epoch [13/20], Loss: 2.3026\n",
            "Epoch [14/20], Loss: 2.3026\n",
            "Epoch [15/20], Loss: 2.3026\n",
            "Epoch [16/20], Loss: 2.3026\n",
            "Epoch [17/20], Loss: 2.3026\n",
            "Epoch [18/20], Loss: 2.3026\n",
            "Epoch [19/20], Loss: 2.3026\n",
            "Epoch [20/20], Loss: 2.3026\n",
            "\n",
            "Accuracy: 10.00%\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model_3 = VGG(num_classes=10).to(device)\n",
        "c_train_loss_list_3, c_metrics_3 = training_and_evaluating(model_3, train_loader, test_loader, 20)\n",
        "c_metrics_3['Dataset']='CIFAR-10'\n",
        "c_metrics_3['Model name']='VGG'\n",
        "pickle.dump(c_train_loss_list_3, open(\"c_train_loss_list_3.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_3, open(\"c_metrics_3.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uncjhp6DHWQO",
        "outputId": "34d0d63f-3955-493e-ca06-c46d99f2a42e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.3605\n",
            "Epoch [2/100], Loss: 0.9384\n",
            "Epoch [3/100], Loss: 0.7409\n",
            "Epoch [4/100], Loss: 0.5750\n",
            "Epoch [5/100], Loss: 0.4349\n",
            "Epoch [6/100], Loss: 0.2928\n",
            "Epoch [7/100], Loss: 0.1809\n",
            "Epoch [8/100], Loss: 0.1015\n",
            "Epoch [9/100], Loss: 0.0542\n",
            "Epoch [10/100], Loss: 0.0316\n",
            "Epoch [11/100], Loss: 0.0196\n",
            "Epoch [12/100], Loss: 0.0130\n",
            "Epoch [13/100], Loss: 0.0096\n",
            "Epoch [14/100], Loss: 0.0089\n",
            "Epoch [15/100], Loss: 0.0070\n",
            "Epoch [16/100], Loss: 0.0074\n",
            "Epoch [17/100], Loss: 0.0063\n",
            "Epoch [18/100], Loss: 0.0053\n",
            "Epoch [19/100], Loss: 0.0055\n",
            "Epoch [20/100], Loss: 0.0044\n",
            "Epoch [21/100], Loss: 0.0050\n",
            "Epoch [22/100], Loss: 0.0040\n",
            "Epoch [23/100], Loss: 0.0043\n",
            "Epoch [24/100], Loss: 0.0047\n",
            "Epoch [25/100], Loss: 0.0041\n",
            "Epoch [26/100], Loss: 0.0045\n",
            "Epoch [27/100], Loss: 0.0046\n",
            "Epoch [28/100], Loss: 0.0041\n",
            "Epoch [29/100], Loss: 0.0039\n",
            "Epoch [30/100], Loss: 0.0046\n",
            "Epoch [31/100], Loss: 0.0041\n",
            "Epoch [32/100], Loss: 0.0047\n",
            "Epoch [33/100], Loss: 0.0047\n",
            "Epoch [34/100], Loss: 0.0045\n",
            "Epoch [35/100], Loss: 0.0045\n",
            "Epoch [36/100], Loss: 0.0049\n",
            "Epoch [37/100], Loss: 0.0041\n",
            "Epoch [38/100], Loss: 0.0040\n",
            "Epoch [39/100], Loss: 0.0044\n",
            "Epoch [40/100], Loss: 0.0047\n",
            "Epoch [41/100], Loss: 0.0044\n",
            "Epoch [42/100], Loss: 0.0046\n",
            "Epoch [43/100], Loss: 0.0045\n",
            "Epoch [44/100], Loss: 0.0046\n",
            "Epoch [45/100], Loss: 0.0050\n",
            "Epoch [46/100], Loss: 0.0045\n",
            "Epoch [47/100], Loss: 0.0038\n",
            "Epoch [48/100], Loss: 0.0042\n",
            "Epoch [49/100], Loss: 0.0041\n",
            "Epoch [50/100], Loss: 0.0042\n",
            "Epoch [51/100], Loss: 0.0046\n",
            "Epoch [52/100], Loss: 0.0039\n",
            "Epoch [53/100], Loss: 0.0047\n",
            "Epoch [54/100], Loss: 0.0046\n",
            "Epoch [55/100], Loss: 0.0037\n",
            "Epoch [56/100], Loss: 0.0043\n",
            "Epoch [57/100], Loss: 0.0039\n",
            "Epoch [58/100], Loss: 0.0043\n",
            "Epoch [59/100], Loss: 0.0046\n",
            "Epoch [60/100], Loss: 0.0042\n",
            "Epoch [61/100], Loss: 0.0040\n",
            "Epoch [62/100], Loss: 0.0045\n",
            "Epoch [63/100], Loss: 0.0045\n",
            "Epoch [64/100], Loss: 0.0039\n",
            "Epoch [65/100], Loss: 0.0045\n",
            "Epoch [66/100], Loss: 0.0046\n",
            "Epoch [67/100], Loss: 0.0045\n",
            "Epoch [68/100], Loss: 0.0040\n",
            "Epoch [69/100], Loss: 0.0050\n",
            "Epoch [70/100], Loss: 0.0040\n",
            "Epoch [71/100], Loss: 0.0041\n",
            "Epoch [72/100], Loss: 0.0039\n",
            "Epoch [73/100], Loss: 0.0046\n",
            "Epoch [74/100], Loss: 0.0046\n",
            "Epoch [75/100], Loss: 0.0039\n",
            "Epoch [76/100], Loss: 0.0049\n",
            "Epoch [77/100], Loss: 0.0041\n",
            "Epoch [78/100], Loss: 0.0045\n",
            "Epoch [79/100], Loss: 0.0041\n",
            "Epoch [80/100], Loss: 0.0047\n",
            "Epoch [81/100], Loss: 0.0045\n",
            "Epoch [82/100], Loss: 0.0043\n",
            "Epoch [83/100], Loss: 0.0039\n",
            "Epoch [84/100], Loss: 0.0045\n",
            "Epoch [85/100], Loss: 0.0042\n",
            "Epoch [86/100], Loss: 0.0042\n",
            "Epoch [87/100], Loss: 0.0042\n",
            "Epoch [88/100], Loss: 0.0036\n",
            "Epoch [89/100], Loss: 0.0045\n",
            "Epoch [90/100], Loss: 0.0054\n",
            "Epoch [91/100], Loss: 0.0044\n",
            "Epoch [92/100], Loss: 0.0037\n",
            "Epoch [93/100], Loss: 0.0045\n",
            "Epoch [94/100], Loss: 0.0040\n",
            "Epoch [95/100], Loss: 0.0037\n",
            "Epoch [96/100], Loss: 0.0041\n",
            "Epoch [97/100], Loss: 0.0043\n",
            "Epoch [98/100], Loss: 0.0042\n",
            "Epoch [99/100], Loss: 0.0047\n",
            "Epoch [100/100], Loss: 0.0046\n",
            "\n",
            "Accuracy: 77.77%\n",
            "Precision: 0.79\n",
            "Recall: 0.79\n",
            "F1-score: 0.79\n"
          ]
        }
      ],
      "source": [
        "model_4 = ResNet18(num_classes=10).to(device)\n",
        "c_train_loss_list_4, c_metrics_4= training_and_evaluating(model_4, train_loader, test_loader, 100)\n",
        "c_metrics_4['Dataset']='CIFAR-10'\n",
        "c_metrics_4['Model name']='ResNet18'\n",
        "pickle.dump(c_train_loss_list_4, open(\"c_train_loss_list_4.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_4, open(\"c_metrics_4.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEsbXwSiHWQP",
        "outputId": "db96ae90-294b-4c91-d967-77769ec59e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.3065\n",
            "Epoch [2/100], Loss: 0.8839\n",
            "Epoch [3/100], Loss: 0.6735\n",
            "Epoch [4/100], Loss: 0.5081\n",
            "Epoch [5/100], Loss: 0.3483\n",
            "Epoch [6/100], Loss: 0.2128\n",
            "Epoch [7/100], Loss: 0.1157\n",
            "Epoch [8/100], Loss: 0.0637\n",
            "Epoch [9/100], Loss: 0.0353\n",
            "Epoch [10/100], Loss: 0.0196\n",
            "Epoch [11/100], Loss: 0.0137\n",
            "Epoch [12/100], Loss: 0.0104\n",
            "Epoch [13/100], Loss: 0.0079\n",
            "Epoch [14/100], Loss: 0.0069\n",
            "Epoch [15/100], Loss: 0.0062\n",
            "Epoch [16/100], Loss: 0.0050\n",
            "Epoch [17/100], Loss: 0.0051\n",
            "Epoch [18/100], Loss: 0.0044\n",
            "Epoch [19/100], Loss: 0.0046\n",
            "Epoch [20/100], Loss: 0.0041\n",
            "Epoch [21/100], Loss: 0.0040\n",
            "Epoch [22/100], Loss: 0.0043\n",
            "Epoch [23/100], Loss: 0.0036\n",
            "Epoch [24/100], Loss: 0.0042\n",
            "Epoch [25/100], Loss: 0.0040\n",
            "Epoch [26/100], Loss: 0.0044\n",
            "Epoch [27/100], Loss: 0.0039\n",
            "Epoch [28/100], Loss: 0.0040\n",
            "Epoch [29/100], Loss: 0.0042\n",
            "Epoch [30/100], Loss: 0.0039\n",
            "Epoch [31/100], Loss: 0.0041\n",
            "Epoch [32/100], Loss: 0.0042\n",
            "Epoch [33/100], Loss: 0.0041\n",
            "Epoch [34/100], Loss: 0.0036\n",
            "Epoch [35/100], Loss: 0.0037\n",
            "Epoch [36/100], Loss: 0.0041\n",
            "Epoch [37/100], Loss: 0.0041\n",
            "Epoch [38/100], Loss: 0.0039\n",
            "Epoch [39/100], Loss: 0.0043\n",
            "Epoch [40/100], Loss: 0.0037\n",
            "Epoch [41/100], Loss: 0.0035\n",
            "Epoch [42/100], Loss: 0.0043\n",
            "Epoch [43/100], Loss: 0.0040\n",
            "Epoch [44/100], Loss: 0.0038\n",
            "Epoch [45/100], Loss: 0.0037\n",
            "Epoch [46/100], Loss: 0.0044\n",
            "Epoch [47/100], Loss: 0.0035\n",
            "Epoch [48/100], Loss: 0.0041\n",
            "Epoch [49/100], Loss: 0.0035\n",
            "Epoch [50/100], Loss: 0.0038\n",
            "Epoch [51/100], Loss: 0.0037\n",
            "Epoch [52/100], Loss: 0.0053\n",
            "Epoch [53/100], Loss: 0.0042\n",
            "Epoch [54/100], Loss: 0.0037\n",
            "Epoch [55/100], Loss: 0.0039\n",
            "Epoch [56/100], Loss: 0.0040\n",
            "Epoch [57/100], Loss: 0.0038\n",
            "Epoch [58/100], Loss: 0.0040\n",
            "Epoch [59/100], Loss: 0.0039\n",
            "Epoch [60/100], Loss: 0.0037\n",
            "Epoch [61/100], Loss: 0.0036\n",
            "Epoch [62/100], Loss: 0.0042\n",
            "Epoch [63/100], Loss: 0.0038\n",
            "Epoch [64/100], Loss: 0.0036\n",
            "Epoch [65/100], Loss: 0.0041\n",
            "Epoch [66/100], Loss: 0.0040\n",
            "Epoch [67/100], Loss: 0.0038\n",
            "Epoch [68/100], Loss: 0.0040\n",
            "Epoch [69/100], Loss: 0.0037\n",
            "Epoch [70/100], Loss: 0.0040\n",
            "Epoch [71/100], Loss: 0.0047\n",
            "Epoch [72/100], Loss: 0.0038\n",
            "Epoch [73/100], Loss: 0.0040\n",
            "Epoch [74/100], Loss: 0.0037\n",
            "Epoch [75/100], Loss: 0.0039\n",
            "Epoch [76/100], Loss: 0.0038\n",
            "Epoch [77/100], Loss: 0.0038\n",
            "Epoch [78/100], Loss: 0.0039\n",
            "Epoch [79/100], Loss: 0.0041\n",
            "Epoch [80/100], Loss: 0.0035\n",
            "Epoch [81/100], Loss: 0.0039\n",
            "Epoch [82/100], Loss: 0.0039\n",
            "Epoch [83/100], Loss: 0.0039\n",
            "Epoch [84/100], Loss: 0.0034\n",
            "Epoch [85/100], Loss: 0.0048\n",
            "Epoch [86/100], Loss: 0.0038\n",
            "Epoch [87/100], Loss: 0.0040\n",
            "Epoch [88/100], Loss: 0.0038\n",
            "Epoch [89/100], Loss: 0.0038\n",
            "Epoch [90/100], Loss: 0.0043\n",
            "Epoch [91/100], Loss: 0.0039\n",
            "Epoch [92/100], Loss: 0.0042\n",
            "Epoch [93/100], Loss: 0.0040\n",
            "Epoch [94/100], Loss: 0.0045\n",
            "Epoch [95/100], Loss: 0.0042\n",
            "Epoch [96/100], Loss: 0.0041\n",
            "Epoch [97/100], Loss: 0.0042\n",
            "Epoch [98/100], Loss: 0.0044\n",
            "Epoch [99/100], Loss: 0.0042\n",
            "Epoch [100/100], Loss: 0.0038\n",
            "\n",
            "Accuracy: 77.46%\n",
            "Precision: 0.78\n",
            "Recall: 0.77\n",
            "F1-score: 0.77\n"
          ]
        }
      ],
      "source": [
        "model_5 = SENet18(num_classes=10).to(device)\n",
        "c_train_loss_list_5, c_metrics_5 = training_and_evaluating(model_5, train_loader, test_loader, 100)\n",
        "c_metrics_5['Dataset']='CIFAR-10'\n",
        "c_metrics_5['Model name']='SENet18'\n",
        "pickle.dump(c_train_loss_list_5, open(\"c_train_loss_list_5.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_5, open(\"c_metrics_5.p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6 = GoogleNet(num_classes=10).to(device)\n",
        "c_train_loss_list_6, c_metrics_6 = training_and_evaluating_GoogleNet(model_6, train_loader, test_loader, 100)\n",
        "c_metrics_6['Dataset']='CIFAR-10'\n",
        "c_metrics_6['Model name']='GoogleNet'\n",
        "pickle.dump(c_train_loss_list_6, open(\"c_train_loss_list_6.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_6, open(\"c_metrics_6.p\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9lnAB6zOTfX",
        "outputId": "ce6ed594-1932-4eca-d025-65478f0af690"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.6362\n",
            "Epoch [2/100], Loss: 1.7372\n",
            "Epoch [3/100], Loss: 1.3056\n",
            "Epoch [4/100], Loss: 0.9902\n",
            "Epoch [5/100], Loss: 0.7114\n",
            "Epoch [6/100], Loss: 0.4633\n",
            "Epoch [7/100], Loss: 0.2603\n",
            "Epoch [8/100], Loss: 0.1287\n",
            "Epoch [9/100], Loss: 0.0530\n",
            "Epoch [10/100], Loss: 0.0211\n",
            "Epoch [11/100], Loss: 0.0097\n",
            "Epoch [12/100], Loss: 0.0057\n",
            "Epoch [13/100], Loss: 0.0041\n",
            "Epoch [14/100], Loss: 0.0033\n",
            "Epoch [15/100], Loss: 0.0029\n",
            "Epoch [16/100], Loss: 0.0024\n",
            "Epoch [17/100], Loss: 0.0021\n",
            "Epoch [18/100], Loss: 0.0019\n",
            "Epoch [19/100], Loss: 0.0018\n",
            "Epoch [20/100], Loss: 0.0018\n",
            "Epoch [21/100], Loss: 0.0018\n",
            "Epoch [22/100], Loss: 0.0017\n",
            "Epoch [23/100], Loss: 0.0016\n",
            "Epoch [24/100], Loss: 0.0017\n",
            "Epoch [25/100], Loss: 0.0015\n",
            "Epoch [26/100], Loss: 0.0015\n",
            "Epoch [27/100], Loss: 0.0015\n",
            "Epoch [28/100], Loss: 0.0015\n",
            "Epoch [29/100], Loss: 0.0015\n",
            "Epoch [30/100], Loss: 0.0015\n",
            "Epoch [31/100], Loss: 0.0015\n",
            "Epoch [32/100], Loss: 0.0015\n",
            "Epoch [33/100], Loss: 0.0015\n",
            "Epoch [34/100], Loss: 0.0015\n",
            "Epoch [35/100], Loss: 0.0015\n",
            "Epoch [36/100], Loss: 0.0015\n",
            "Epoch [37/100], Loss: 0.0016\n",
            "Epoch [38/100], Loss: 0.0015\n",
            "Epoch [39/100], Loss: 0.0015\n",
            "Epoch [40/100], Loss: 0.0016\n",
            "Epoch [41/100], Loss: 0.0016\n",
            "Epoch [42/100], Loss: 0.0015\n",
            "Epoch [43/100], Loss: 0.0015\n",
            "Epoch [44/100], Loss: 0.0015\n",
            "Epoch [45/100], Loss: 0.0015\n",
            "Epoch [46/100], Loss: 0.0015\n",
            "Epoch [47/100], Loss: 0.0015\n",
            "Epoch [48/100], Loss: 0.0016\n",
            "Epoch [49/100], Loss: 0.0015\n",
            "Epoch [50/100], Loss: 0.0015\n",
            "Epoch [51/100], Loss: 0.0015\n",
            "Epoch [52/100], Loss: 0.0015\n",
            "Epoch [53/100], Loss: 0.0015\n",
            "Epoch [54/100], Loss: 0.0015\n",
            "Epoch [55/100], Loss: 0.0016\n",
            "Epoch [56/100], Loss: 0.0015\n",
            "Epoch [57/100], Loss: 0.0016\n",
            "Epoch [58/100], Loss: 0.0015\n",
            "Epoch [59/100], Loss: 0.0015\n",
            "Epoch [60/100], Loss: 0.0014\n",
            "Epoch [61/100], Loss: 0.0015\n",
            "Epoch [62/100], Loss: 0.0015\n",
            "Epoch [63/100], Loss: 0.0016\n",
            "Epoch [64/100], Loss: 0.0016\n",
            "Epoch [65/100], Loss: 0.0015\n",
            "Epoch [66/100], Loss: 0.0015\n",
            "Epoch [67/100], Loss: 0.0016\n",
            "Epoch [68/100], Loss: 0.0016\n",
            "Epoch [69/100], Loss: 0.0016\n",
            "Epoch [70/100], Loss: 0.0016\n",
            "Epoch [71/100], Loss: 0.0015\n",
            "Epoch [72/100], Loss: 0.0015\n",
            "Epoch [73/100], Loss: 0.0014\n",
            "Epoch [74/100], Loss: 0.0015\n",
            "Epoch [75/100], Loss: 0.0015\n",
            "Epoch [76/100], Loss: 0.0015\n",
            "Epoch [77/100], Loss: 0.0015\n",
            "Epoch [78/100], Loss: 0.0015\n",
            "Epoch [79/100], Loss: 0.0014\n",
            "Epoch [80/100], Loss: 0.0016\n",
            "Epoch [81/100], Loss: 0.0016\n",
            "Epoch [82/100], Loss: 0.0015\n",
            "Epoch [83/100], Loss: 0.0015\n",
            "Epoch [84/100], Loss: 0.0015\n",
            "Epoch [85/100], Loss: 0.0016\n",
            "Epoch [86/100], Loss: 0.0015\n",
            "Epoch [87/100], Loss: 0.0015\n",
            "Epoch [88/100], Loss: 0.0016\n",
            "Epoch [89/100], Loss: 0.0016\n",
            "Epoch [90/100], Loss: 0.0015\n",
            "Epoch [91/100], Loss: 0.0016\n",
            "Epoch [92/100], Loss: 0.0015\n",
            "Epoch [93/100], Loss: 0.0015\n",
            "Epoch [94/100], Loss: 0.0015\n",
            "Epoch [95/100], Loss: 0.0015\n",
            "Epoch [96/100], Loss: 0.0015\n",
            "Epoch [97/100], Loss: 0.0015\n",
            "Epoch [98/100], Loss: 0.0016\n",
            "Epoch [99/100], Loss: 0.0015\n",
            "Epoch [100/100], Loss: 0.0015\n",
            "\n",
            "Accuracy: 76.00%\n",
            "Precision: 0.75\n",
            "Recall: 0.74\n",
            "F1-score: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pouf7MsbAUxQ"
      },
      "source": [
        "**Modified VGGNet Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3Gz8fMR4IJX4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucRmLMlkk9iD"
      },
      "outputs": [],
      "source": [
        "#Data_loader for MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['MNIST'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['MNIST'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST\n",
        "model_3 = VGGNet(num_classes=10).to(device)\n",
        "m_train_loss_list_3_modified, m_metrics_3_modified = training_and_evaluating(model_3, train_loader, test_loader, 20)\n",
        "pickle.dump(m_train_loss_list_3_modified, open(\"m_train_loss_list_3_modified.p\", \"wb\"))\n",
        "pickle.dump(m_metrics_3_modified, open(\"m_metrics_3_modified.p\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbUBIVUm-Hkb",
        "outputId": "2b2d4a02-7d2b-41ec-fd57-119003833692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6717\n",
            "Epoch [2/20], Loss: 0.1240\n",
            "Epoch [3/20], Loss: 0.0668\n",
            "Epoch [4/20], Loss: 0.0547\n",
            "Epoch [5/20], Loss: 0.0404\n",
            "Epoch [6/20], Loss: 0.0321\n",
            "Epoch [7/20], Loss: 0.0266\n",
            "Epoch [8/20], Loss: 0.0218\n",
            "Epoch [9/20], Loss: 0.0165\n",
            "Epoch [10/20], Loss: 0.0153\n",
            "Epoch [11/20], Loss: 0.0126\n",
            "Epoch [12/20], Loss: 0.0120\n",
            "Epoch [13/20], Loss: 0.0102\n",
            "Epoch [14/20], Loss: 0.0104\n",
            "Epoch [15/20], Loss: 0.0083\n",
            "Epoch [16/20], Loss: 0.0090\n",
            "Epoch [17/20], Loss: 0.0083\n",
            "Epoch [18/20], Loss: 0.0085\n",
            "Epoch [19/20], Loss: 0.0085\n",
            "Epoch [20/20], Loss: 0.0081\n",
            "\n",
            "Accuracy: 99.64%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ZDOgL-Dk9iE"
      },
      "outputs": [],
      "source": [
        "#Data_loader for FASHION MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['FMNIST'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['FMNIST'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fashion MNIST\n",
        "model_3 = VGGNet(num_classes=10).to(device)\n",
        "f_train_loss_list_3_modified, f_metrics_3_modified = training_and_evaluating(model_3, train_loader, test_loader, 20)\n",
        "pickle.dump(f_train_loss_list_3_modified, open(\"f_train_loss_list_3_modified.p\", \"wb\"))\n",
        "pickle.dump(f_metrics_3_modified, open(\"f_metrics_3_modified.p\", \"wb\"))"
      ],
      "metadata": {
        "id": "qgy5PhTJK5TD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d6e5ad-9e81-42a6-a6dc-4af5b29d2183"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.2399\n",
            "Epoch [2/20], Loss: 0.5594\n",
            "Epoch [3/20], Loss: 0.4091\n",
            "Epoch [4/20], Loss: 0.3442\n",
            "Epoch [5/20], Loss: 0.3039\n",
            "Epoch [6/20], Loss: 0.2749\n",
            "Epoch [7/20], Loss: 0.2469\n",
            "Epoch [8/20], Loss: 0.2281\n",
            "Epoch [9/20], Loss: 0.2118\n",
            "Epoch [10/20], Loss: 0.2007\n",
            "Epoch [11/20], Loss: 0.1926\n",
            "Epoch [12/20], Loss: 0.1864\n",
            "Epoch [13/20], Loss: 0.1810\n",
            "Epoch [14/20], Loss: 0.1783\n",
            "Epoch [15/20], Loss: 0.1744\n",
            "Epoch [16/20], Loss: 0.1710\n",
            "Epoch [17/20], Loss: 0.1725\n",
            "Epoch [18/20], Loss: 0.1720\n",
            "Epoch [19/20], Loss: 0.1723\n",
            "Epoch [20/20], Loss: 0.1696\n",
            "\n",
            "Accuracy: 92.42%\n",
            "Precision: 0.93\n",
            "Recall: 0.93\n",
            "F1-score: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhfmM0mVk9iF"
      },
      "outputs": [],
      "source": [
        "#Data_loader for CIFAR-10 DATA\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(data['CIFAR-10'][0], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(data['CIFAR-10'][1], batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDnEQSFdS92_",
        "outputId": "9252f2cb-9713-49e8-e2aa-26e5f9222f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 2.2908\n",
            "Epoch [2/100], Loss: 1.9514\n",
            "Epoch [3/100], Loss: 1.6058\n",
            "Epoch [4/100], Loss: 1.3205\n",
            "Epoch [5/100], Loss: 1.1517\n",
            "Epoch [6/100], Loss: 1.0170\n",
            "Epoch [7/100], Loss: 0.9121\n",
            "Epoch [8/100], Loss: 0.8396\n",
            "Epoch [9/100], Loss: 0.7862\n",
            "Epoch [10/100], Loss: 0.7476\n",
            "Epoch [11/100], Loss: 0.7231\n",
            "Epoch [12/100], Loss: 0.7018\n",
            "Epoch [13/100], Loss: 0.6980\n",
            "Epoch [14/100], Loss: 0.6803\n",
            "Epoch [15/100], Loss: 0.6765\n",
            "Epoch [16/100], Loss: 0.6745\n",
            "Epoch [17/100], Loss: 0.6719\n",
            "Epoch [18/100], Loss: 0.6635\n",
            "Epoch [19/100], Loss: 0.6682\n",
            "Epoch [20/100], Loss: 0.6667\n",
            "Epoch [21/100], Loss: 0.6621\n",
            "Epoch [22/100], Loss: 0.6673\n",
            "Epoch [23/100], Loss: 0.6628\n",
            "Epoch [24/100], Loss: 0.6598\n",
            "Epoch [25/100], Loss: 0.6593\n",
            "Epoch [26/100], Loss: 0.6630\n",
            "Epoch [27/100], Loss: 0.6665\n",
            "Epoch [28/100], Loss: 0.6602\n",
            "Epoch [29/100], Loss: 0.6586\n",
            "Epoch [30/100], Loss: 0.6610\n",
            "Epoch [31/100], Loss: 0.6574\n",
            "Epoch [32/100], Loss: 0.6650\n",
            "Epoch [33/100], Loss: 0.6571\n",
            "Epoch [34/100], Loss: 0.6612\n",
            "Epoch [35/100], Loss: 0.6608\n",
            "Epoch [36/100], Loss: 0.6626\n",
            "Epoch [37/100], Loss: 0.6622\n",
            "Epoch [38/100], Loss: 0.6613\n",
            "Epoch [39/100], Loss: 0.6606\n",
            "Epoch [40/100], Loss: 0.6640\n",
            "Epoch [41/100], Loss: 0.6655\n",
            "Epoch [42/100], Loss: 0.6594\n",
            "Epoch [43/100], Loss: 0.6580\n",
            "Epoch [44/100], Loss: 0.6663\n",
            "Epoch [45/100], Loss: 0.6611\n",
            "Epoch [46/100], Loss: 0.6616\n",
            "Epoch [47/100], Loss: 0.6645\n",
            "Epoch [48/100], Loss: 0.6625\n",
            "Epoch [49/100], Loss: 0.6617\n",
            "Epoch [50/100], Loss: 0.6639\n",
            "Epoch [51/100], Loss: 0.6634\n",
            "Epoch [52/100], Loss: 0.6583\n",
            "Epoch [53/100], Loss: 0.6595\n",
            "Epoch [54/100], Loss: 0.6560\n",
            "Epoch [55/100], Loss: 0.6613\n",
            "Epoch [56/100], Loss: 0.6614\n",
            "Epoch [57/100], Loss: 0.6638\n",
            "Epoch [58/100], Loss: 0.6610\n",
            "Epoch [59/100], Loss: 0.6624\n",
            "Epoch [60/100], Loss: 0.6643\n",
            "Epoch [61/100], Loss: 0.6615\n",
            "Epoch [62/100], Loss: 0.6601\n",
            "Epoch [63/100], Loss: 0.6596\n",
            "Epoch [64/100], Loss: 0.6646\n",
            "Epoch [65/100], Loss: 0.6612\n",
            "Epoch [66/100], Loss: 0.6600\n",
            "Epoch [67/100], Loss: 0.6585\n",
            "Epoch [68/100], Loss: 0.6608\n",
            "Epoch [69/100], Loss: 0.6598\n",
            "Epoch [70/100], Loss: 0.6599\n",
            "Epoch [71/100], Loss: 0.6634\n",
            "Epoch [72/100], Loss: 0.6584\n",
            "Epoch [73/100], Loss: 0.6593\n",
            "Epoch [74/100], Loss: 0.6632\n",
            "Epoch [75/100], Loss: 0.6618\n",
            "Epoch [76/100], Loss: 0.6605\n",
            "Epoch [77/100], Loss: 0.6613\n",
            "Epoch [78/100], Loss: 0.6638\n",
            "Epoch [79/100], Loss: 0.6624\n",
            "Epoch [80/100], Loss: 0.6616\n",
            "Epoch [81/100], Loss: 0.6650\n",
            "Epoch [82/100], Loss: 0.6582\n",
            "Epoch [83/100], Loss: 0.6646\n",
            "Epoch [84/100], Loss: 0.6622\n",
            "Epoch [85/100], Loss: 0.6611\n",
            "Epoch [86/100], Loss: 0.6605\n",
            "Epoch [87/100], Loss: 0.6606\n",
            "Epoch [88/100], Loss: 0.6562\n",
            "Epoch [89/100], Loss: 0.6601\n",
            "Epoch [90/100], Loss: 0.6607\n",
            "Epoch [91/100], Loss: 0.6593\n",
            "Epoch [92/100], Loss: 0.6599\n",
            "Epoch [93/100], Loss: 0.6645\n",
            "Epoch [94/100], Loss: 0.6627\n",
            "Epoch [95/100], Loss: 0.6586\n",
            "Epoch [96/100], Loss: 0.6589\n",
            "Epoch [97/100], Loss: 0.6552\n",
            "Epoch [98/100], Loss: 0.6651\n",
            "Epoch [99/100], Loss: 0.6628\n",
            "Epoch [100/100], Loss: 0.6636\n",
            "\n",
            "Accuracy: 76.57%\n",
            "Precision: 0.77\n",
            "Recall: 0.76\n",
            "F1-score: 0.76\n"
          ]
        }
      ],
      "source": [
        "#CIFAR-10\n",
        "model_3 = VGGNet(num_classes=10).to(device)\n",
        "c_train_loss_list_3_modified, c_metrics_3_modified = training_and_evaluating(model_3, train_loader, test_loader, 100)\n",
        "pickle.dump(c_train_loss_list_3_modified, open(\"c_train_loss_list_3_modified.p\", \"wb\"))\n",
        "pickle.dump(c_metrics_3_modified, open(\"c_metrics_3_modified.p\", \"wb\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}